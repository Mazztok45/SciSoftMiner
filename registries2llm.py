# -*- coding: utf-8 -*-
"""Registries2LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_UZLsyw9QWXKb45yWb3EE6t6enH3olW5
"""

import requests
from pathlib import Path
from typing import List, Dict, Tuple, Set, Any, Optional

class APIClient:
    """Base class for API interactions with common functionality."""

    def __init__(self, base_url: str, headers: Optional[Dict] = None):
        self.base_url = base_url
        self.headers = headers or {"Content-Type": "application/json"}

    def get(self, endpoint: str, params: Optional[Dict] = None) -> Dict:
        """Make a GET request to the API."""
        url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"
        response = requests.get(url, headers=self.headers, params=params)
        response.raise_for_status()
        return response.json()

    def download_file(self, url: str, output_path: Path, chunk_size: int = 8192) -> Path:
        """Download a file from a URL."""
        response = requests.get(url, headers=self.headers, stream=True)
        response.raise_for_status()

        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=chunk_size):
                f.write(chunk)
        return output_path

class InfomaniakDriveClient(APIClient):
    """Client for Infomaniak Drive API."""

    def __init__(self, token: str, drive_id: str):
        headers = {"Authorization": f"Bearer {token}"}
        super().__init__("https://api.infomaniak.com/3/drive", headers)
        self.drive_id = drive_id

    def list_folder_contents(self, folder_id: str) -> List[Dict]:
        """List all files in a folder."""
        endpoint = f"{self.drive_id}/files/{folder_id}/files"
        response = self.get(endpoint)
        return response.get('data', [])

    def download_folder_files(self, folder_id: str, output_dir: Path = Path(".")) -> List[Path]:
        """Download all files from a folder."""
        files = self.list_folder_contents(folder_id)
        downloaded_files = []

        for item in files:
            if item['type'] == 'file':
                file_id = item['id']
                file_name = item['name']
                download_url = f"https://api.infomaniak.com/2/drive/{self.drive_id}/files/{file_id}/download"

                print(f"Downloading {file_name}...")
                output_path = output_dir / file_name
                self.download_file(download_url, output_path)
                downloaded_files.append(output_path)
                print(f"âœ“ Downloaded {file_name}")

        return downloaded_files

    def download_file_by_id(self, file_id: str, output_name: str) -> Path:
        """Download a specific file by ID."""
        download_url = f"https://api.infomaniak.com/2/drive/{self.drive_id}/files/{file_id}/download"
        print(f"Downloading {output_name}...")
        return self.download_file(download_url, Path(output_name))

infomaniak = InfomaniakDriveClient(
        token="Fi9Aaadhibox6l50TcideMovSUo8JtaKc30i_yP-XJFcf_db9HL_BX9KQ0HZN41H8u53Pu6tGRq3PGx5",
        drive_id="705884"
    )

infomaniak.download_file_by_id("20632", "full_contrastive_new_math_discoveries.json")

infomaniak.download_file_by_id("20560", "math_blobs.tar.gz")

infomaniak.download_file_by_id("20631", "full_contrastive_new_physics_discoveries.json")

infomaniak.download_file_by_id("20561", "physics_blobs.tar.gz")

infomaniak.download_file_by_id("20630", "full_contrastive_new_biology_discoveries.json")

infomaniak.download_file_by_id("20562", "biology_blobs.tar.gz")

import tarfile
import os

def extract_tar_gz(tar_file, extract_dir=None):
    """
    Simple function to extract a tar.gz file

    Args:
        tar_file: Path to the .tar.gz file
        extract_dir: Where to extract (defaults to same folder as tar file)
    """
    # Set default extract directory
    if extract_dir is None:
        extract_dir = os.path.dirname(tar_file) or "."

    # Make sure extract directory exists
    os.makedirs(extract_dir, exist_ok=True)

    # Extract
    with tarfile.open(tar_file, "r:gz") as tar:
        tar.extractall(path=extract_dir)

    print(f"Extracted {tar_file} to {extract_dir}")

extract_tar_gz("/content/math_blobs.tar.gz")

extract_tar_gz("/content/physics_blobs.tar.gz")

extract_tar_gz("/content/biology_blobs.tar.gz")

"""### LLM SECTION


"""

# -*- coding: utf-8 -*-
"""Refactored Minimal Domain Classifier"""

import json
import os
import re
import torch
import time
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Dict, Optional
from multiprocessing import Pool, cpu_count
from pathlib import Path
from huggingface_hub import login

# Log in with your token
login(token="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")
class BlobCollector:
    """Minimal blob file reader."""

    @staticmethod
    def read_blob_content(blob_path: str) -> Optional[str]:
        if not os.path.exists(blob_path):
            return None
        try:
            with open(blob_path, 'rb') as f:
                content = f.read().decode('utf-8', errors='ignore')
                return content if len(content.strip()) > 10 else None
        except:
            return None

    @staticmethod
    def read_blob_contents_parallel(blob_paths: List[str]) -> List[Optional[str]]:
        num_workers = min(cpu_count(), 8)
        with Pool(num_workers) as pool:
            return list(pool.map(BlobCollector.read_blob_content, blob_paths))

class DomainClassifier:
    """Unified classifier for all domains."""

    DOMAIN_PROMPTS = {
        'mathematics': {
            'scale': """0=No math, 1=Minimal, 2=Some, 3=Moderate, 4=High, 5=Pure math""",
            'topics': 'statistics, algorithms, ML, optimization, proofs, equations'
        },
        'physics': {
            'scale': """0=No physics, 1=Minimal, 2=Some, 3=Moderate, 4=High, 5=Pure physics""",
            'topics': 'mechanics, quantum, astrophysics, particles, energy, simulations'
        },
        'biology': {
            'scale': """0=No biology, 1=Minimal, 2=Some, 3=Moderate, 4=High, 5=Pure biology""",
            'topics': 'genetics, medicine, ecology, bioinformatics, cells, organisms'
        }
    }

    def __init__(self, model_name: str = "microsoft/phi-2"):
        self.model_name = model_name

        # Model mapping
        model_map = {
            "qwen": "Qwen/Qwen2.5-1.5B-Instruct",
            "phi2": "microsoft/phi-2",
            "hf": "HuggingFaceTB/SmolLM3-3B"
        }

        actual_model = model_map.get(model_name.lower(), model_name)

        print(f"Loading {actual_model}")
        self.tokenizer = AutoTokenizer.from_pretrained(actual_model, trust_remote_code=True, padding_side='left')

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        self.model = AutoModelForCausalLM.from_pretrained(
            actual_model,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        self.model.eval()

    def batch_classify(self, texts: List[str], domain: str = 'mathematics', batch_size: int = 16) -> List[Optional[int]]:
        if not texts:
            return []

        # Filter valid texts
        valid_texts = []
        valid_indices = []
        for i, text in enumerate(texts):
            if text and len(text.strip()) >= 10:
                valid_texts.append(text)
                valid_indices.append(i)

        if not valid_texts:
            return [None] * len(texts)

        # Domain prompt
        domain_info = self.DOMAIN_PROMPTS.get(domain, self.DOMAIN_PROMPTS['mathematics'])

        # Create prompts
        prompts = []
        for text in valid_texts:
            cleaned = text[:600].replace('`', '').strip()
            cleaned = re.sub(r'http\S+', '', cleaned)
            cleaned = re.sub(r'\s+', ' ', cleaned)

            # Model-specific prompts
            if "qwen" in self.model_name.lower():
                prompt = f"""<|im_start|>system
You are a {domain} domain classifier. Rate content 0-5.<|im_end|>
<|im_start|>user
Scale: {domain_info['scale']}
Topics: {domain_info['topics']}
Text: {cleaned}
Rating (0-5):<|im_end|>
<|im_start|>assistant
"""
            elif "phi" in self.model_name.lower():
                prompt = f"""Rate {domain} content 0-5:
Scale: {domain_info['scale']}
Text: {cleaned}
Rating:"""
            else:  # Default/StableLM format
                prompt = f"""<s>[INST] Rate {domain} content 0-5:
{domain_info['scale']}
Text: {cleaned}
Output only number: [/INST] Rating:"""

            prompts.append(prompt)

        ratings = [None] * len(texts)

        # Batch process
        for batch_start in range(0, len(prompts), batch_size):
            batch_end = min(batch_start + batch_size, len(prompts))
            batch_prompts = prompts[batch_start:batch_end]
            batch_indices = valid_indices[batch_start:batch_end]

            inputs = self.tokenizer(
                batch_prompts,
                return_tensors="pt",
                truncation=True,
                max_length=768,
                padding=True
            )

            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=2,
                    temperature=0.01,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            for i, output in enumerate(outputs):
                input_length = inputs['input_ids'][i].shape[0]
                response = self.tokenizer.decode(output[input_length:], skip_special_tokens=True).strip()
                rating = self._extract_rating(response)
                ratings[batch_indices[i]] = rating

        return ratings

    def _extract_rating(self, text: str) -> Optional[int]:
        digits = re.findall(r'\b(\d)\b', text)
        for digit in digits:
            if digit.isdigit():
                rating = int(digit)
                if 0 <= rating <= 5:
                    return rating
        for char in text:
            if char.isdigit():
                rating = int(char)
                if 0 <= rating <= 5:
                    return rating
        return None

class RepositoryProcessor:
    """Process repositories for all domains."""

    def __init__(self, model_name: str = "phi2", batch_size: int = 16, num_workers: int = 8):
        self.model_name = model_name
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.classifier = DomainClassifier(model_name)

        # Domain files mapping
        self.domain_files = {
            'mathematics': 'full_contrastive_new_math_discoveries.json',
            'physics': 'full_contrastive_new_physics_discoveries.json',
            'biology': 'full_contrastive_new_biology_discoveries.json'
        }

    def load_repositories(self, domain: str) -> List[Dict]:
        """Load repositories from downloaded JSON file."""
        file_path = Path(self.domain_files[domain])
        if not file_path.exists():
            print(f"File not found: {file_path}")
            return []

        with open(file_path, 'r') as f:
            data = json.load(f)

        # Extract repositories from the JSON structure
        repos = []
        if isinstance(data, dict):
            # Try common keys
            for key in ['repositories', 'items', 'data']:
                if key in data:
                    repos = data[key]
                    break
            if not repos and 'results' in data:
                repos = data['results']
        elif isinstance(data, list):
            repos = data

        print(f"Loaded {len(repos)} {domain} repositories")
        return repos[:5000] if repos else []  # Limit to 5000 for testing

    def process_domain(self, domain: str):
        """Process a single domain."""
        print(f"\nProcessing {domain}...")

        # Load repositories
        repos = self.load_repositories(domain)
        if not repos:
            return

        # Extract blob paths
        blob_paths = []
        valid_indices = []
        for i, repo in enumerate(repos):
            readme = repo.get('readme') or repo.get('readme_content') or repo.get('description', '')
            if readme and isinstance(readme, str) and len(readme) > 10:
                # For actual blob paths, you'd extract the hash
                # For now, we'll use the text directly
                repos[i]['readme_content'] = readme
                repos[i]['readme_exists'] = True
            else:
                repos[i]['readme_exists'] = False

        # Get texts for classification
        texts = []
        for repo in repos:
            if repo.get('readme_exists'):
                text = repo.get('readme_content', '')
                texts.append(text if len(text.strip()) >= 10 else '')
            else:
                texts.append('')

        # Classify
        print(f"Classifying {len([t for t in texts if t])} texts...")
        ratings = self.classifier.batch_classify(texts, domain, self.batch_size)

        # Combine results
        results = []
        for i, (repo, rating) in enumerate(zip(repos, ratings)):
            result = repo.copy()

            # Get repo name
            url = str(repo.get('url', repo.get('html_url', '')))
            name = "Unknown"
            if 'github.com' in url:
                parts = url.split('github.com/')[-1].split('/')
                if len(parts) >= 2:
                    name = f"{parts[0]}/{parts[1]}"

            result['name'] = name
            result['domain'] = domain

            if repo.get('readme_exists'):
                result['readme_preview'] = repo.get('readme_content', '')[:200].replace('\n', ' ')
                result['llm_rating'] = rating
                result[f'{domain}_rating'] = rating if rating is not None else 0
                result['rating_source'] = 'llm' if rating is not None else 'failed'
            else:
                result[f'{domain}_rating'] = 0
                result['rating_source'] = 'no_readme'

            results.append(result)

        # Save results
        timestamp = int(time.time())
        model_basename = self.model_name.split("/")[-1]
        output_file = f"{model_basename}_{domain}_{len(repos)}_{timestamp}.jsonl"

        with open(output_file, 'w') as f:
            for result in results:
                f.write(json.dumps(result) + '\n')

        # Generate summary
        self._generate_summary(results, domain, output_file)

        return results

    def _generate_summary(self, results: List[Dict], domain: str, output_file: str):
        repos_with_readme = [r for r in results if r.get('readme_exists', False)]
        if not repos_with_readme:
            return

        ratings = [r.get(f'{domain}_rating', 0) for r in repos_with_readme]
        avg_rating = sum(ratings) / len(ratings)

        from collections import Counter
        rating_counts = Counter(ratings)

        print(f"\n{domain.upper()} SUMMARY")
        print(f"Processed: {len(results)} repos")
        print(f"With README: {len(repos_with_readme)} ({len(repos_with_readme)/len(results)*100:.1f}%)")
        print(f"Average rating: {avg_rating:.2f}/5")

        print("\nDistribution:")
        for rating in range(6):
            count = rating_counts.get(rating, 0)
            percentage = (count / len(ratings)) * 100 if ratings else 0
            print(f"  {rating}/5: {count} repos ({percentage:.1f}%)")

        print(f"\nSaved to: {output_file}")

    def run_all_domains(self):
        """Run classification for all three domains."""
        domains = ['mathematics', 'physics', 'biology']
        all_results = {}

        for domain in domains:
            results = self.process_domain(domain)
            all_results[domain] = results

        print("\n" + "="*60)
        print("ALL DOMAINS PROCESSING COMPLETE")
        print("="*60)

        return all_results

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "phi2"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "qwen"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "hf"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "mistralai/Mistral-7B-v0.1"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "Qwen/Qwen2-7B-Instruct"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "google/gemma-7b-it"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "NousResearch/Nous-Hermes-2-Mistral-7B-DPO"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "Intel/neural-chat-7b-v3-3"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "HuggingFaceH4/zephyr-7b-beta"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "google/gemma-2-9b-it"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "Qwen/Qwen2.5-7B-Instruct"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

rm -rf ~/.cache/huggingface/hub

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "meta-llama/Llama-2-7b-hf"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "deepseek-ai/deepseek-llm-7b-base"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "meta-llama/Llama-3.1-8B"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "AIDC-AI/Marco-o1"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "upstage/SOLAR-10.7B-Instruct-v1.0"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

rm -rf ~/.cache/huggingface/hub

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "NousResearch/Hermes-2-Pro-Llama-3-8B"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "mistralai/Mistral-7B-Instruct-v0.3"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "mistralai/Mistral-Nemo-Instruct-2407"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "Qwen/Qwen2.5-14B-Instruct"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

# Clear PyTorch cache
torch.cuda.empty_cache()
import gc
gc.collect()

# Kill all processes using GPU (most effective)
!fuser -v /dev/nvidia* 2>/dev/null | xargs -I{} kill -9 {} 2>/dev/null || true

# Alternative: kill all python processes
!killall -9 python 2>/dev/null || true

!nvidia-smi -pm 0

!nvidia-smi -r

# First, clear PyTorch's cache
torch.cuda.empty_cache()

# Force garbage collection
gc.collect()

# Reset PyTorch's memory allocator (if using CUDA 10.2+)
try:
    from torch.cuda import memory
    memory.caching_allocator_alloc()
    memory.caching_allocator_delete()
except:
    pass

# Main execution
if __name__ == "__main__":
    # Configure here
    MODEL = "Qwen/Qwen2.5-72B-Instruct"  # Options: "phi2", "qwen", "hf", or any HuggingFace model ID
    BATCH_SIZE = 16
    NUM_WORKERS = 8

    # Initialize and run
    processor = RepositoryProcessor(
        model_name=MODEL,
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS
    )

    # Run for all domains
    processor.run_all_domains()

