# -*- coding: utf-8 -*-
"""Keywords2Software.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ff0PwHIWv8jrAag1KJUaaTKUa6WC8-Xi
"""

# -*- coding: utf-8 -*-
"""MultiDomainSoftwareReposFinder.ipynb

Automatically generated pipeline for finding mathematical, physics, and biology software repositories.
"""

# ===============================
# IMPORTS & SETUP
# ===============================
import os
import json
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
import gdown
import orjson
import math
import warnings
import tarfile
import zstandard as zstd
import requests
from pathlib import Path
from typing import List, Dict, Tuple, Set, Any, Optional
from collections import Counter
from tqdm.auto import tqdm
from multiprocessing import Pool, cpu_count
from sentence_transformers import SentenceTransformer
warnings.filterwarnings('ignore')

# Your credentials
TOKEN = "Fi9Aaadhibox6l50TcideMovSUo8JtaKc30i_yP-XJFcf_db9HL_BX9KQ0HZN41H8u53Pu6tGRq3PGx5"
DRIVE_ID = "705884"
DOWNLOAD_FOLDER_ID = "20476"  # Ontology2Keywords folder
UPLOAD_FOLDER_ID = "20521"   # Folder to upload output files

headers = {
    "Authorization": f"Bearer {TOKEN}",
    "Content-Type": "application/json"
}

# ===============================
# INFOMANIAK DRIVE HELPER
# ===============================
class InfomaniakDriveHelper:
    """Helper class for Infomaniak Drive operations."""

    @staticmethod
    def list_folder_contents(folder_id):
        """List all contents of a folder using v3 API"""
        url = f"https://api.infomaniak.com/3/drive/{DRIVE_ID}/files/{folder_id}/files"

        all_items = []
        cursor = None

        try:
            while True:
                params = {}
                if cursor:
                    params['cursor'] = cursor

                response = requests.get(url, headers=headers, params=params)
                response.raise_for_status()
                data = response.json()

                if data.get('result') == 'success':
                    items = data.get('data', [])
                    all_items.extend(items)

                    # Check if there are more pages
                    if data.get('has_more', False):
                        cursor = data.get('cursor')
                    else:
                        break
                else:
                    print(f"API error: {data}")
                    break

        except Exception as e:
            print(f"Error listing folder: {e}")

        return all_items

    @staticmethod
    def download_file(file_id, file_name, destination_path):
        """Download a single file using v2 API"""
        print(f"  ðŸ“„ Downloading: {file_name}")

        download_url = f"https://api.infomaniak.com/2/drive/{DRIVE_ID}/files/{file_id}/download"

        try:
            response = requests.get(download_url, headers=headers, stream=True, allow_redirects=True)
            response.raise_for_status()

            # Get file size from headers
            total_size = int(response.headers.get('content-length', 0))

            # Save file
            file_path = Path(destination_path) / file_name

            with open(file_path, 'wb') as f:
                if total_size == 0:
                    f.write(response.content)
                    print(f"    âœ“ Downloaded")
                else:
                    downloaded = 0
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                            # Show progress for files > 1MB
                            if total_size > 1024 * 1024:
                                percent = (downloaded / total_size) * 100
                                if downloaded % (1024 * 1024) < 8192:  # Update every ~1MB
                                    print(f"    Progress: {percent:.1f}%", end='\r')

                    print(f"    âœ“ Downloaded: {downloaded:,} bytes")

            return True

        except requests.exceptions.RequestException as e:
            print(f"    âœ— Network error: {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"    Status code: {e.response.status_code}")
                print(f"    Response: {e.response.text[:200]}")
            return False
        except Exception as e:
            print(f"    âœ— Error: {e}")
            return False

    @staticmethod
    def upload_file(file_path, folder_id):
        """Upload a file to Infomaniak Drive."""
        file_name = Path(file_path).name
        print(f"ðŸ“¤ Uploading: {file_name}")

        upload_url = f"https://api.infomaniak.com/3/drive/{DRIVE_ID}/upload"

        try:
            with open(file_path, 'rb') as f:
                file_content = f.read()

            params = {
                "total_size": len(file_content),
                "directory_id": folder_id,
                "file_name": file_name
            }

            response = requests.post(upload_url, headers=headers, params=params, data=file_content)
            response.raise_for_status()

            result = response.json()
            if result.get('result') == 'success':
                print(f"    âœ“ Uploaded successfully")
                return True
            else:
                print(f"    âœ— Upload failed: {result}")
                return False

        except Exception as e:
            print(f"    âœ— Upload error: {e}")
            return False

    @staticmethod
    def download_required_files():
        """Download all required files from the Ontology2Keywords folder."""
        print("=" * 70)
        print("DOWNLOADING REQUIRED FILES FROM INFOMANIAK DRIVE")
        print("=" * 70)

        # List folder contents
        items = InfomaniakDriveHelper.list_folder_contents(DOWNLOAD_FOLDER_ID)

        if not items:
            print("No files found in folder or cannot access folder.")
            return []

        # Filter for files only
        files_to_download = []
        for item in items:
            if item.get('type') == 'file':
                files_to_download.append({
                    'id': item.get('id'),
                    'name': item.get('name')
                })

        if not files_to_download:
            print("No files to download.")
            return []

        print(f"ðŸ“ Found {len(files_to_download)} files to download:")

        # Create output directory
        output_dir = Path("downloaded_files")
        output_dir.mkdir(exist_ok=True)

        # Download files
        downloaded_files = []
        for file_info in files_to_download:
            file_id = file_info['id']
            file_name = file_info['name']

            if InfomaniakDriveHelper.download_file(file_id, file_name, output_dir):
                downloaded_files.append(output_dir / file_name)

        print(f"\nâœ… Downloaded {len(downloaded_files)} files to {output_dir.absolute()}")
        return downloaded_files

# ===============================
# CONFIGURATION
# ===============================
class DomainConfig:
    """Configuration for different scientific domains."""

    DOMAINS = {
        'math': {
            'name': 'Mathematics',
            'term_file': 'mathematics_keywords.txt',
            'context_prefix': 'mathematical concept: ',
            'negative_context_prefix': 'software topic: ',
            'keywords': {None},
            'contrastive_thresholds': {
                'margin_min': 0.2,
                'ratio_min': 1.2,
                'ensemble_min': 0.3
            }
        },
        'physics': {
            'name': 'Physics',
            'term_file': 'physics_keywords.txt',
            'context_prefix': 'physics concept: ',
            'negative_context_prefix': 'software topic: ',
            'keywords': {None},
            'contrastive_thresholds': {
                'margin_min': 0.2,
                'ratio_min': 1.2,
                'ensemble_min': 0.3
            }
        },
        'biology': {
            'name': 'Biology',
            'term_file': 'biology_keywords.txt',
            'context_prefix': 'biological concept: ',
            'negative_context_prefix': 'software topic: ',
            'keywords': {None},
            'contrastive_thresholds': {
                'margin_min': 0.2,
                'ratio_min': 1.2,
                'ensemble_min': 0.3
            }
        }
    }

    @classmethod
    def get_domain_config(cls, domain: str) -> Dict:
        """Get configuration for a specific domain."""
        if domain not in cls.DOMAINS:
            raise ValueError(f"Domain must be one of: {list(cls.DOMAINS.keys())}")
        return cls.DOMAINS[domain]

# ===============================
# DATA DOWNLOADER
# ===============================
class DataDownloader:
    """Handles downloading of required data files."""

    @staticmethod
    def download_github_topics(file_id: str = '1uVFHDOBfzPeNKehnq2grByJQ4U5Sm4nH',
                              output_file: str = 'repos.ndjson') -> str:
        """Download GitHub topics dataset."""
        print(f"Downloading GitHub topics dataset...")
        gdown.download(id=file_id, output=output_file, quiet=False)
        print(f"File downloaded as {output_file}")
        return output_file

    @staticmethod
    def download_blobs(file_id: str = '1qqXGHWbBPjQOj8-H2-HaFSgpabPPK__p',
                      output_file: str = 'blobs-by-swhid.tar.zst') -> str:
        """Download and extract blob data."""
        print(f"Downloading blob data...")
        gdown.download(id=file_id, output=output_file, quiet=False)

        # Decompress .zst file
        zst_output_file = output_file.replace('.zst', '')
        print(f"Decompressing {output_file}...")
        with zstd.open(output_file, 'rb') as compressed_file:
            with open(zst_output_file, 'wb') as decompressed_file:
                decompressed_file.write(compressed_file.read())

        # Extract .tar file
        extraction_dir = "extracted_data"
        os.makedirs(extraction_dir, exist_ok=True)
        print(f"Extracting {zst_output_file}...")
        with tarfile.open(zst_output_file, 'r') as tar:
            tar.extractall(path=extraction_dir)

        print("Extraction complete.")
        return extraction_dir

# ===============================
# DOMAIN TERM LOADER
# ===============================
class DomainTermLoader:
    """Loads and processes domain-specific terms."""

    @staticmethod
    def load_terms(term_file: str) -> List[str]:
        """Load domain terms from file."""
        try:
            # Check if file exists in downloaded_files directory
            downloaded_path = Path("downloaded_files") / term_file
            if downloaded_path.exists():
                file_path = downloaded_path
            elif Path(term_file).exists():
                file_path = term_file
            else:
                print(f"Warning: Term file {term_file} not found.")
                return []

            with open(file_path, "r") as f:
                content = f.read()
                # Handle different separators
                if "/n" in content:
                    terms = [term.strip() for term in content.split("/n") if term.strip()]
                else:
                    terms = [term.strip() for term in content.split("\n") if term.strip()]

            print(f"Loaded {len(terms)} terms from {term_file}")
            print(f"First 5 terms: {terms[:5]}")
            return terms
        except FileNotFoundError:
            print(f"Warning: Term file {term_file} not found. Using default terms.")
            return []
        except Exception as e:
            print(f"Error loading terms from {term_file}: {e}")
            return []

# ===============================
# EMBEDDING MODEL
# ===============================
class EmbeddingModel:
    """Handles embedding model loading and encoding."""

    def __init__(self, device: str = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None
        self.load_model()

    def load_model(self):
        """Load the embedding model."""
        print(f"Using device: {self.device}")

        try:
            print("Loading SPECTER2 model...")
            from transformers import AutoTokenizer, AutoModel
            from adapters import AutoAdapterModel

            self.tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')
            model = AutoAdapterModel.from_pretrained('allenai/specter2_base')
            adapter_name = model.load_adapter("allenai/specter2", source="hf", set_active=True)
            self.model = model.to(self.device)
            self.model.eval()
            print("SPECTER2 loaded successfully!")

        except Exception as e:
            print(f"Error loading SPECTER2: {e}")


    def encode(self, texts: List[str], batch_size: int = 256,
               convert_to_tensor: bool = True, **kwargs) -> torch.Tensor:
        """Encode texts to embeddings."""
        if not texts:
            raise ValueError("Cannot encode empty list of texts")

        if hasattr(self.model, 'encode') and callable(self.model.encode):
            # For SentenceTransformer models
            result = self.model.encode(
                texts,
                convert_to_tensor=convert_to_tensor,
                show_progress_bar=True,
                batch_size=batch_size,
                normalize_embeddings=True
            )
            if convert_to_tensor and not isinstance(result, torch.Tensor):
                result = torch.tensor(result, device=self.device)
            return result.to(self.device) if convert_to_tensor else result
        else:
            # For SPECTER2 models
            self.model.eval()
            all_embeddings = []

            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512
                ).to(self.device)

                with torch.no_grad():
                    outputs = self.model(**inputs)
                    embeddings = outputs.last_hidden_state[:, 0, :]
                    embeddings = F.normalize(embeddings, p=2, dim=1)
                    all_embeddings.append(embeddings.cpu())

            result = torch.cat(all_embeddings)
            return result.to(self.device) if convert_to_tensor else result

# ===============================
# CONTRASTIVE FILTERING
# ===============================
class ContrastiveFiltering:
    """Enhanced contrastive filtering for domain-specific topics."""

    def __init__(self, domain_config: Dict, embedding_model: EmbeddingModel):
        self.domain_config = domain_config
        self.encoder = embedding_model
        self.domain_embeddings = None
        self.negative_embeddings = None
        self.domain_centroid = None
        self.negative_centroid = None

    def create_negative_samples(self, all_topics: List[str],
                               topic_counter: Counter,
                               domain_terms: List[str]) -> List[str]:
        """Create negative samples using embedding similarity instead of keywords."""
        print("Creating negative samples using embedding similarity...")

        # Encode domain terms once
        if self.domain_embeddings is None:
            self.encode_domain_terms(domain_terms)

        # Get most common topics
        most_common = topic_counter.most_common(500)
        candidate_topics = [topic for topic, _ in most_common]

        # Encode candidate topics
        candidate_embeddings = self.encoder.encode(
            [f"topic: {topic}" for topic in candidate_topics],
            batch_size=128,
            convert_to_tensor=True
        )

        # Compute similarity to domain centroid
        similarities = F.cosine_similarity(
            candidate_embeddings,
            self.domain_centroid
        ).cpu().numpy()

        # Select topics with LOW similarity to domain
        negative_indices = np.argsort(similarities)[:200]  # 200 least similar

        negative_samples = [candidate_topics[i] for i in negative_indices]

        print(f"Selected {len(negative_samples)} negative samples using embedding similarity")
        print(f"Sample negatives: {negative_samples[:10]}")

        return negative_samples

    def encode_domain_terms(self, domain_terms: List[str]) -> torch.Tensor:
        """Encode domain terms with context augmentation."""
        print(f"Encoding {len(domain_terms)} domain terms...")
        context_prefix = self.domain_config['context_prefix']
        domain_terms_with_context = [f"{context_prefix}{term}" for term in domain_terms]
        embeddings = self.encoder.encode(domain_terms_with_context, batch_size=128)
        self.domain_embeddings = embeddings
        self.domain_centroid = embeddings.mean(dim=0, keepdim=True)
        return embeddings

    def encode_negative_samples(self, negative_samples: List[str]) -> torch.Tensor:
        """Encode negative samples with context."""
        print(f"Encoding {len(negative_samples)} negative samples...")
        context_prefix = self.domain_config.get('negative_context_prefix', 'software topic: ')
        negative_terms_with_context = [f"{context_prefix}{term}" for term in negative_samples]
        embeddings = self.encoder.encode(negative_terms_with_context, batch_size=128)
        self.negative_embeddings = embeddings
        self.negative_centroid = embeddings.mean(dim=0, keepdim=True)
        return embeddings

    def enhanced_contrastive_scoring(self, topic_embedding: torch.Tensor,
                                   temperature: float = 0.07) -> Dict:
        """Enhanced contrastive scoring with attention-weighted similarities."""
        topic_emb_norm = F.normalize(topic_embedding, p=2, dim=1)
        domain_emb_norm = F.normalize(self.domain_embeddings, p=2, dim=1)
        neg_emb_norm = F.normalize(self.negative_embeddings, p=2, dim=1)

        # Compute attention weights for domain terms
        domain_similarities = torch.mm(topic_emb_norm, domain_emb_norm.T)
        domain_weights = F.softmax(domain_similarities / temperature, dim=1)
        weighted_domain_sim = (domain_similarities * domain_weights).sum(dim=1)

        # Compute attention weights for negative terms
        neg_similarities = torch.mm(topic_emb_norm, neg_emb_norm.T)
        neg_weights = F.softmax(neg_similarities / temperature, dim=1)
        weighted_neg_sim = (neg_similarities * neg_weights).sum(dim=1)

        # Centroid similarities
        domain_centroid_sim = F.cosine_similarity(topic_emb_norm, self.domain_centroid).squeeze()
        neg_centroid_sim = F.cosine_similarity(topic_emb_norm, self.negative_centroid).squeeze()

        # Enhanced contrastive margin
        margin = (weighted_domain_sim + 0.3 * domain_centroid_sim) - (weighted_neg_sim + 0.3 * neg_centroid_sim)

        # Compute ratio with smoothing
        ratio = (weighted_domain_sim + 1e-9) / (weighted_neg_sim + 1e-9)

        # Diversity score
        if domain_similarities.size(1) > 1:
            domain_diversity = domain_similarities.std(dim=1).mean().item()
        else:
            domain_diversity = 0

        return {
            'weighted_domain_sim': weighted_domain_sim.item(),
            'weighted_neg_sim': weighted_neg_sim.item(),
            'margin': margin.item(),
            'ratio': ratio.item(),
            'domain_diversity': domain_diversity,
            'domain_centroid_sim': domain_centroid_sim.item(),
            'neg_centroid_sim': neg_centroid_sim.item()
        }

    def filter_topics(self, all_topics: List[str], topic_counter: Counter,
                 domain_terms: List[str]) -> List[Dict]:
      """Main filtering pipeline."""
      print(f"\n{'='*70}")
      print(f"Processing {len(all_topics)} topics for {self.domain_config['name']}")
      print('='*70)

      # Step 1: Create negative samples - ADD domain_terms parameter
      negative_samples = self.create_negative_samples(all_topics, topic_counter, domain_terms)  # CHANGED

      # Step 2: Encode domain terms and negative samples
      self.encode_domain_terms(domain_terms)
      self.encode_negative_samples(negative_samples)

      # Step 3: Process all topics
      domain_related_topics = []
      total_topics = len(all_topics)
      chunk_size = 50000
      num_chunks = (total_topics + chunk_size - 1) // chunk_size

      for chunk_idx in range(num_chunks):
          start_idx = chunk_idx * chunk_size
          end_idx = min((chunk_idx + 1) * chunk_size, total_topics)

          print(f"\nProcessing chunk {chunk_idx + 1}/{num_chunks} (topics {start_idx}-{end_idx})")

          chunk_topics = all_topics[start_idx:end_idx]
          chunk_batch_size = min(512, len(chunk_topics))

          pbar = tqdm(total=len(chunk_topics), desc=f"Chunk {chunk_idx + 1}")

          for topic_start in range(0, len(chunk_topics), chunk_batch_size):
              topic_end = min(topic_start + chunk_batch_size, len(chunk_topics))
              batch_topics = chunk_topics[topic_start:topic_end]

              # Encode topics with context
              batch_topics_with_context = [f"topic: {topic}" for topic in batch_topics]
              topic_embeddings = self.encoder.encode(
                  batch_topics_with_context,
                  convert_to_tensor=True,
                  show_progress_bar=False,
                  batch_size=128
              )

              # Compute scores for each topic
              for i, topic in enumerate(batch_topics):
                  topic_emb = topic_embeddings[i:i+1]

                  scores = self.enhanced_contrastive_scoring(
                      topic_emb,
                      temperature=0.05
                  )

                  # Find closest domain term
                  similarities = torch.mm(F.normalize(topic_emb, p=2, dim=1),
                                        F.normalize(self.domain_embeddings, p=2, dim=1).T)
                  closest_idx = similarities.argmax().item()
                  confidence = similarities.mean().item()

                  domain_related_topics.append({
                      'topic': topic,
                      'weighted_domain_sim': scores['weighted_domain_sim'],
                      'weighted_neg_sim': scores['weighted_neg_sim'],
                      'contrastive_margin': scores['margin'],
                      'domain_neg_ratio': scores['ratio'],
                      'domain_diversity': scores['domain_diversity'],
                      'closest_domain_term': domain_terms[closest_idx],
                      'similarity_confidence': confidence,
                      'domain_centroid_sim': scores['domain_centroid_sim'],
                      'neg_centroid_sim': scores['neg_centroid_sim'],
                      'domain': self.domain_config['name']
                  })

              pbar.update(len(batch_topics))

              # Memory management
              del topic_embeddings
              if torch.cuda.is_available():
                  torch.cuda.empty_cache()

          pbar.close()

      return domain_related_topics
# ===============================
# MULTI-STAGE FILTERING
# ===============================
class MultiStageFilter:
    """Multi-stage adaptive filtering with statistical analysis."""

    @staticmethod
    def filter_topics(domain_related_topics: List[Dict],
                     threshold_strategy: str = 'percentile') -> List[Dict]:
        """Apply multi-stage filtering."""
        if not domain_related_topics:
            return []

        print("Stage 1: Statistical analysis...")

        # Extract metrics
        margins = np.array([t['contrastive_margin'] for t in domain_related_topics])
        ratios = np.array([t['domain_neg_ratio'] for t in domain_related_topics])
        weighted_sims = np.array([t['weighted_domain_sim'] for t in domain_related_topics])
        confidences = np.array([t['similarity_confidence'] for t in domain_related_topics])

        print(f"  Margin stats: mean={margins.mean():.3f}, std={margins.std():.3f}")
        print(f"  Ratio stats: mean={ratios.mean():.2f}, std={ratios.std():.2f}")

        # Adaptive thresholding
        if threshold_strategy == 'percentile':
            margin_threshold = np.percentile(margins, 80)
            ratio_threshold = np.percentile(ratios, 75)
            sim_threshold = np.percentile(weighted_sims, 75)
            conf_threshold = np.percentile(confidences, 70)
        else:
            margin_threshold = margins.mean() + margins.std() * 0.5
            ratio_threshold = ratios.mean() + ratios.std() * 0.5
            sim_threshold = weighted_sims.mean() + weighted_sims.std() * 0.5
            conf_threshold = confidences.mean() + confidences.std() * 0.5

        print(f"\nStage 2: Applying adaptive thresholds...")
        print(f"  Margin threshold: {margin_threshold:.3f}")
        print(f"  Ratio threshold: {ratio_threshold:.2f}")

        # Filter topics
        filtered_topics = []
        for t in domain_related_topics:
            passes_criteria = (
                (t['contrastive_margin'] >= margin_threshold and t['domain_neg_ratio'] >= 1.5) or
                (t['weighted_domain_sim'] >= sim_threshold and t['similarity_confidence'] >= conf_threshold) or
                (t['domain_neg_ratio'] >= max(ratio_threshold, 2.0) and t['contrastive_margin'] >= 0)
            )

            if passes_criteria:
                # Calculate ensemble score
                z_margin = (t['contrastive_margin'] - margins.mean()) / (margins.std() + 1e-9)
                z_ratio = (t['domain_neg_ratio'] - ratios.mean()) / (ratios.std() + 1e-9)
                z_sim = (t['weighted_domain_sim'] - weighted_sims.mean()) / (weighted_sims.std() + 1e-9)

                t['ensemble_score'] = 0.4 * z_margin + 0.3 * z_ratio + 0.3 * z_sim
                filtered_topics.append(t)

        print(f"  Topics after filtering: {len(filtered_topics)} / {len(domain_related_topics)}")

        # Sort by ensemble score
        filtered_topics.sort(key=lambda x: x.get('ensemble_score', 0), reverse=True)

        return filtered_topics

# ===============================
# REPOSITORY FINDER
# ===============================
class RepositoryFinder:
    """Finds repositories related to specific domains."""

    def __init__(self, domain: str, filtered_topics: List[Dict]):
        self.domain = domain
        self.filtered_topics = filtered_topics
        self.strict_topic_set = self.create_strict_topic_set()
        self.topic_idf_scores = None
        self.math_topic_set = None  # For compatibility with existing code

    def create_strict_topic_set(self) -> Set[str]:
        """Create strict set of domain topics based on contrastive metrics."""
        strict_topics = set()
        thresholds = DomainConfig.get_domain_config(self.domain)['contrastive_thresholds']

        for topic_data in self.filtered_topics:
            if (topic_data['contrastive_margin'] > thresholds['margin_min'] and
                topic_data['domain_neg_ratio'] > thresholds['ratio_min'] and
                topic_data.get('ensemble_score', 0) > thresholds['ensemble_min']):
                strict_topics.add(topic_data['topic'])

        print(f"Created strict topic set with {len(strict_topics)} topics")
        return strict_topics

    def compute_idf_scores(self, filename: str = 'repos.ndjson',
                          n_workers: int = None) -> Dict[str, float]:
        """Compute IDF scores for all topics."""
        n_workers = n_workers or cpu_count()
        n_chunks = n_workers * 4

        print("Computing IDF scores...")

        # Read file in chunks
        chunks, total_lines = self._read_file_chunks(filename, n_chunks)

        # Count topics in parallel
        with Pool(n_workers) as pool:
            topic_counters = list(tqdm(
                pool.imap_unordered(self._count_topics_in_chunk, chunks),
                total=len(chunks),
                desc="Counting topics"
            ))

        # Merge counters and compute IDF
        all_topics_counter = Counter()
        for counter in topic_counters:
            all_topics_counter.update(counter)

        idf_scores = {}
        for topic, doc_freq in all_topics_counter.items():
            idf_scores[topic] = math.log(total_lines / doc_freq)

        self.topic_idf_scores = idf_scores
        print(f"Computed IDF for {len(idf_scores):,} unique topics")
        return idf_scores

    def find_repositories(self, filename: str = 'repos.ndjson',
                         n_workers: int = None) -> List[Dict]:
        """Find repositories containing domain topics."""
        n_workers = n_workers or cpu_count()
        n_chunks = n_workers * 4

        if self.topic_idf_scores is None:
            self.compute_idf_scores(filename, n_workers)

        print(f"\nFinding {self.domain} repositories...")

        # Read file in chunks
        chunks, total_lines = self._read_file_chunks(filename, n_chunks)

        # Process in parallel
        with Pool(n_workers, initializer=self._init_worker,
                  initargs=(self.topic_idf_scores, self.strict_topic_set)) as pool:
            results = list(tqdm(
                pool.imap_unordered(self._process_chunk, chunks),
                total=len(chunks),
                desc=f"Finding {self.domain} repos"
            ))

        # Merge results
        all_matching = []
        domain_topics_counter = Counter()

        for matching, counter in results:
            all_matching.extend(matching)
            domain_topics_counter.update(counter)

        # Analyze results
        print(f"\n{'='*60}")
        print(f"{self.domain.upper()} REPOSITORY FINDING RESULTS")
        print('='*60)
        print(f"Total matching repositories: {len(all_matching):,}")
        print(f"Percentage of total: {len(all_matching)/total_lines*100:.2f}%")

        print(f"\nTop 20 detected {self.domain} topics:")
        for topic, count in domain_topics_counter.most_common(20):
            print(f"  {topic}: {count:,}")

        return all_matching

    def save_results(self, matching_repos: List[Dict],
                    output_prefix: str = None):
        """Save results to files and upload to Infomaniak Drive."""
        if output_prefix is None:
            output_prefix = self.domain

        # Save matching repositories
        output_file = f"{output_prefix}_repos.jsonl"
        with open(output_file, 'w', encoding='utf-8') as f:
            for repo in matching_repos:
                f.write(orjson.dumps(repo).decode('utf-8') + '\n')

        # Save filtered topics
        topics_file = f"{output_prefix}_filtered_topics.json"
        with open(topics_file, 'w') as f:
            json.dump(self.filtered_topics, f, indent=2)

        # Save strict topics
        strict_topics_file = f"{output_prefix}_topics.txt"
        with open(strict_topics_file, 'w') as f:
            for topic in sorted(self.strict_topic_set):
                f.write(f"{topic}\n")

        # Upload files to Infomaniak Drive
        files_to_upload = [output_file, topics_file, strict_topics_file]
        for file_path in files_to_upload:
            if os.path.exists(file_path):
                InfomaniakDriveHelper.upload_file(file_path, UPLOAD_FOLDER_ID)

        print(f"\nSaved and uploaded results:")
        for file_path in files_to_upload:
            if os.path.exists(file_path):
                print(f"  - {file_path}")

    # Helper methods for parallel processing
    @staticmethod
    def _read_file_chunks(filename: str, num_chunks: int) -> Tuple[List[List[str]], int]:
        """Read file and split into chunks."""
        print("Loading file into memory...")
        with open(filename, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()

        total_lines = len(all_lines)
        chunk_size = total_lines // num_chunks + 1

        chunks = []
        for i in range(0, total_lines, chunk_size):
            chunks.append(all_lines[i:i + chunk_size])

        return chunks, total_lines

    @staticmethod
    def _count_topics_in_chunk(lines: List[str]) -> Counter:
        """Count topics in a chunk."""
        counter = Counter()
        for line in lines:
            try:
                data = orjson.loads(line.strip())
                topics = data.get('topics', [])
                if topics:
                    counter.update(topics)
            except:
                pass
        return counter

    @staticmethod
    def _process_chunk(lines: List[str]) -> Tuple[List[Dict], Counter]:
        """Process chunk to find domain repositories."""
        matching = []
        topic_counter = Counter()

        global _worker_idf_scores, _worker_topic_set

        for line in lines:
            try:
                data = orjson.loads(line.strip())
                topics = data.get('topics', [])

                if not topics:
                    continue

                # Get only domain topics
                domain_topics = [t for t in topics if t in _worker_topic_set]

                if not domain_topics:
                    continue

                # Calculate scores
                domain_score = sum(_worker_idf_scores.get(t, 0) for t in domain_topics)
                total_score = sum(_worker_idf_scores.get(t, 0) for t in topics)

                if total_score > 0:
                    domain_ratio = domain_score / total_score

                    # Adaptive threshold
                    num_topics = len(topics)
                    if num_topics <= 3:
                        threshold = 0.4
                    elif num_topics <= 6:
                        threshold = 0.35
                    else:
                        threshold = 0.3

                    if domain_ratio >= threshold:
                        matching.append({
                            'swhid': data.get('swhid'),
                            #'readme': data.get('readme'),
                            'topics': topics,
                            'url': data.get('url', []),
                            'domain_topics': domain_topics,
                            'domain_ratio': round(domain_ratio, 3),
                            'num_topics': num_topics,
                            'domain': _worker_topic_set.domain if hasattr(_worker_topic_set, 'domain') else 'unknown'
                        })
                        topic_counter.update(domain_topics)
            except:
                pass

        return matching, topic_counter

    @staticmethod
    def _init_worker(idf_dict: Dict, topic_set: Set):
        """Initialize worker process."""
        global _worker_idf_scores, _worker_topic_set
        _worker_idf_scores = idf_dict
        _worker_topic_set = topic_set

# ===============================
# BLOB COLLECTOR
# ===============================
class BlobCollector:
    """Collects blob files for matching repositories."""

    @staticmethod
    def build_blob_paths(matching_repos: List[Dict],
                        extraction_dir: str = "extracted_data") -> List[str]:
        """Build blob file paths from matching repositories."""
        blob_paths = []

        for repo in matching_repos:
            readme = repo.get('readme')
            if readme:
                # Extract hash from readme (format: swh:1:cnt:hash)
                hash_part = readme.split(':')[-1]

                if hash_part and len(hash_part) >= 4:
                    first_two = hash_part[:2]
                    next_two = hash_part[2:4]

                    blob_pattern = f"{extraction_dir}/blobs-by-swhid/{first_two}/{next_two}/{hash_part}"
                    blob_paths.append(blob_pattern)

        print(f"Built {len(blob_paths)} blob paths")
        return blob_paths

    @staticmethod
    def collect_and_upload(blob_paths: List[str], output_filename: str):
        """Collect files and upload to Infomaniak Drive."""
        # Collect existing files
        files = [path for path in blob_paths if os.path.exists(path)]

        if not files:
            print("No files found to collect")
            return

        # Create tar.gz archive
        print(f"Creating archive {output_filename}...")
        with tarfile.open(output_filename, "w:gz") as tar:
            for file in files:
                tar.add(file, arcname=os.path.relpath(file, start="extracted_data"))

        # Upload to Infomaniak Drive
        InfomaniakDriveHelper.upload_file(output_filename, UPLOAD_FOLDER_ID)

# ===============================
# MAIN PIPELINE
# ===============================
class MultiDomainPipeline:
    """Main pipeline for processing multiple scientific domains."""

    def __init__(self, domains: List[str] = None):
        if domains is None:
            domains = ['math', 'physics', 'biology']
        self.domains = domains
        self.results = {}

    def run(self):
        """Run the complete pipeline for all domains."""
        print("="*70)
        print("MULTI-DOMAIN SOFTWARE REPOSITORY PIPELINE")
        print("="*70)

        # Step 1: Download required files from Infomaniak Drive
        print("\n[STEP 1] Downloading required files from Infomaniak Drive...")
        downloaded_files = InfomaniakDriveHelper.download_required_files()

        if not downloaded_files:
            print("Warning: No files were downloaded. Pipeline may fail if required files are missing.")

        # Step 2: Download GitHub topics dataset
        print("\n[STEP 2] Downloading GitHub topics dataset...")
        downloader = DataDownloader()
        repos_file = downloader.download_github_topics()

        # Step 3: Download blob data
        print("\n[STEP 3] Downloading blob data...")
        extraction_dir = downloader.download_blobs()

        # Step 4: Load all topics from GitHub data
        print("\n[STEP 4] Loading GitHub topics...")
        all_topics, topic_counter = self._load_all_topics(repos_file)

        # Step 5: Initialize embedding model
        print("\n[STEP 5] Initializing embedding model...")
        embedding_model = EmbeddingModel()

        # Step 6: Process each domain
        for domain in self.domains:
            print(f"\n{'='*70}")
            print(f"PROCESSING DOMAIN: {domain.upper()}")
            print('='*70)

            try:
                domain_results = self._process_domain(
                    domain, all_topics, topic_counter, embedding_model
                )
                self.results[domain] = domain_results

                print(f"\nâœ… Completed {domain.upper()} processing!")
                print(f"   Found {len(domain_results['filtered_topics'])} domain topics")
                print(f"   Found {len(domain_results['matching_repos'])} repositories")

            except Exception as e:
                print(f"âŒ Error processing {domain}: {e}")
                import traceback
                traceback.print_exc()
                continue

        # Step 7: Generate combined report
        self._generate_report()

        # Step 8: Collect and upload blobs for each domain
        print("\n[STEP 8] Collecting and uploading blob files...")
        for domain, results in self.results.items():
            if results and 'matching_repos' in results:
                blob_paths = BlobCollector.build_blob_paths(
                    results['matching_repos'], extraction_dir
                )
                output_filename = f"{domain}_blobs.tar.gz"
                BlobCollector.collect_and_upload(blob_paths, output_filename)

        print("\n" + "="*70)
        print("PIPELINE COMPLETE!")
        print("="*70)

    def _load_all_topics(self, repos_file: str) -> Tuple[List[str], Counter]:
        """Load all topics from the GitHub dataset."""
        topic_counter = Counter()

        with open(repos_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="Loading topics"):
                try:
                    data = orjson.loads(line.strip())
                    topics = data.get('topics', [])
                    if topics:
                        topic_counter.update(topics)
                except:
                    pass

        all_topics = list(topic_counter.keys())
        print(f"Loaded {len(all_topics)} unique topics")
        return all_topics, topic_counter

    def _process_domain(self, domain: str, all_topics: List[str],
                       topic_counter: Counter, embedding_model: EmbeddingModel) -> Dict:
        """Process a single domain."""
        config = DomainConfig.get_domain_config(domain)

        # Load domain terms
        term_loader = DomainTermLoader()
        domain_terms = term_loader.load_terms(config['term_file'])

        if not domain_terms:
            print(f"Warning: No terms loaded for {domain}")
            return {}

        # Apply contrastive filtering
        contrastive_filter = ContrastiveFiltering(config, embedding_model)
        domain_related_topics = contrastive_filter.filter_topics(
            all_topics, topic_counter, domain_terms
        )

        # Apply multi-stage filtering
        filtered_topics = MultiStageFilter.filter_topics(domain_related_topics)

        # Save filtered topics
        output_file = f"{domain}_filtered_topics.json"
        with open(output_file, 'w') as f:
            json.dump(filtered_topics, f, indent=2)

        # Find repositories
        repo_finder = RepositoryFinder(domain, filtered_topics)
        matching_repos = repo_finder.find_repositories()

        # Save and upload repository results
        repo_finder.save_results(matching_repos)

        return {
            'config': config,
            'domain_terms': domain_terms,
            'filtered_topics': filtered_topics,
            'matching_repos': matching_repos
        }

    def _generate_report(self):
        """Generate a comprehensive report of all domains."""
        print("\n" + "="*70)
        print("COMBINED RESULTS REPORT")
        print("="*70)

        for domain, results in self.results.items():
            if not results:
                continue

            filtered_topics = results.get('filtered_topics', [])
            matching_repos = results.get('matching_repos', [])

            print(f"\n{domain.upper()}:")
            print(f"  - Filtered topics: {len(filtered_topics)}")
            print(f"  - Matching repositories: {len(matching_repos)}")

            if filtered_topics:
                print(f"  - Top 5 topics:")
                for i, topic in enumerate(filtered_topics[:5]):
                    print(f"    {i+1}. {topic['topic']} (score: {topic.get('ensemble_score', 0):.3f})")
# ===============================
# EXECUTION
# ===============================
if __name__ == "__main__":
    # Install required packages
    try:
        import pip
        print("Installing required packages...")
        pip.main(['install', 'git+https://github.com/bio-ontology-research-group/mowl'])
        pip.main(['install', 'keybert[flair]'])
        pip.main(['install', 'zstandard'])

        # Install Java if needed
        import subprocess
        subprocess.run(['apt-get', 'update'], check=False)
        subprocess.run(['apt-get', 'install', '-y', 'openjdk-11-jdk'], check=False)

        # Set JAVA_HOME
        os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"

    except Exception as e:
        print(f"Note: Some package installations may have failed: {e}")
        print("Continuing with existing packages...")

    # Configure which domains to process
    domains_to_process = ['math', 'physics', 'biology']  # Adjust as needed

    # Create and run pipeline
    pipeline = MultiDomainPipeline(domains=domains_to_process)
    pipeline.run()

import requests
from pathlib import Path
from typing import List, Dict, Tuple, Set, Any, Optional

class APIClient:
    """Base class for API interactions with common functionality."""

    def __init__(self, base_url: str, headers: Optional[Dict] = None):
        self.base_url = base_url
        self.headers = headers or {"Content-Type": "application/json"}

    def get(self, endpoint: str, params: Optional[Dict] = None) -> Dict:
        """Make a GET request to the API."""
        url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"
        response = requests.get(url, headers=self.headers, params=params)
        response.raise_for_status()
        return response.json()

    def download_file(self, url: str, output_path: Path, chunk_size: int = 8192) -> Path:
        """Download a file from a URL."""
        response = requests.get(url, headers=self.headers, stream=True)
        response.raise_for_status()

        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=chunk_size):
                f.write(chunk)
        return output_path

class InfomaniakDriveClient(APIClient):
    """Client for Infomaniak Drive API."""

    def __init__(self, token: str, drive_id: str):
        headers = {"Authorization": f"Bearer {token}"}
        super().__init__("https://api.infomaniak.com/3/drive", headers)
        self.drive_id = drive_id

    def list_folder_contents(self, folder_id: str) -> List[Dict]:
        """List all files in a folder."""
        endpoint = f"{self.drive_id}/files/{folder_id}/files"
        response = self.get(endpoint)
        return response.get('data', [])

    def download_folder_files(self, folder_id: str, output_dir: Path = Path(".")) -> List[Path]:
        """Download all files from a folder."""
        files = self.list_folder_contents(folder_id)
        downloaded_files = []

        for item in files:
            if item['type'] == 'file':
                file_id = item['id']
                file_name = item['name']
                download_url = f"https://api.infomaniak.com/2/drive/{self.drive_id}/files/{file_id}/download"

                print(f"Downloading {file_name}...")
                output_path = output_dir / file_name
                self.download_file(download_url, output_path)
                downloaded_files.append(output_path)
                print(f"âœ“ Downloaded {file_name}")

        return downloaded_files

    def download_file_by_id(self, file_id: str, output_name: str) -> Path:
        """Download a specific file by ID."""
        download_url = f"https://api.infomaniak.com/2/drive/{self.drive_id}/files/{file_id}/download"
        print(f"Downloading {output_name}...")
        return self.download_file(download_url, Path(output_name))

infomaniak = InfomaniakDriveClient(
        token="Fi9Aaadhibox6l50TcideMovSUo8JtaKc30i_yP-XJFcf_db9HL_BX9KQ0HZN41H8u53Pu6tGRq3PGx5",
        drive_id="705884"
    )

# Download files from Infomaniak
print("Downloading files from Infomaniak Drive...")
downloaded_files = infomaniak.download_folder_files(folder_id="20476")
math_repos_path = infomaniak.download_file_by_id("20525", "mathematics_keywords.txt")

import gdown
class DataDownloader:
    """Handles downloading of required data files."""

    @staticmethod
    def download_github_topics(file_id: str = '1uVFHDOBfzPeNKehnq2grByJQ4U5Sm4nH',
                              output_file: str = 'repos.ndjson') -> str:
        """Download GitHub topics dataset."""
        print(f"Downloading GitHub topics dataset...")
        gdown.download(id=file_id, output=output_file, quiet=False)
        print(f"File downloaded as {output_file}")
        return output_file
DataDownloader.download_github_topics()

# -*- coding: utf-8 -*-
"""MultiDomainSoftwareReposFinder.ipynb

Automatically generated pipeline for finding mathematical, physics, and biology software repositories.
"""
# ===============================
# BASELINE 1
# ===============================
# ===============================
# IMPORTS & SETUP
# ===============================
import os
import json
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
import math
import warnings
import tarfile
import zstandard as zstd
from pathlib import Path
from typing import List, Dict, Tuple, Set, Any, Optional
from collections import Counter
from tqdm.auto import tqdm
from multiprocessing import Pool, cpu_count
from sentence_transformers import SentenceTransformer
warnings.filterwarnings('ignore')

# ===============================
# CONFIGURATION
# ===============================
class DomainConfig:
    """Configuration for different scientific domains."""

    DOMAINS = {
        'math': {
            'name': 'Mathematics',
            'term_file': 'mathematics_keywords.txt',
            'context_prefix': 'mathematical concept: ',
            'similarity_threshold': 0.6,
            'min_topics': 1000,
            'max_topics': 5000
        },
        'physics': {
            'name': 'Physics',
            'term_file': 'physics_keywords.txt',
            'context_prefix': 'physics concept: ',
            'similarity_threshold': 0.6,
            'min_topics': 1000,
            'max_topics': 5000
        },
        'biology': {
            'name': 'Biology',
            'term_file': 'biology_keywords.txt',
            'context_prefix': 'biological concept: ',
            'similarity_threshold': 0.6,
            'min_topics': 1000,
            'max_topics': 5000
        }
    }

    @classmethod
    def get_domain_config(cls, domain: str) -> Dict:
        """Get configuration for a specific domain."""
        if domain not in cls.DOMAINS:
            raise ValueError(f"Domain must be one of: {list(cls.DOMAINS.keys())}")
        return cls.DOMAINS[domain]

# ===============================
# DOMAIN TERM LOADER
# ===============================
class DomainTermLoader:
    """Loads and processes domain-specific terms."""

    @staticmethod
    def load_terms(term_file: str) -> List[str]:
        """Load domain terms from file."""
        try:
            # Try multiple possible locations
            possible_paths = [
                Path(term_file),
                Path("downloaded_files") / term_file,
                Path("data") / term_file,
                Path(".") / term_file
            ]

            file_path = None
            for path in possible_paths:
                if path.exists():
                    file_path = path
                    break

            if file_path is None:
                print(f"Warning: Term file {term_file} not found in any expected location.")
                return []

            with open(file_path, "r", encoding='utf-8') as f:
                content = f.read()
                # Handle different separators
                if "/n" in content:
                    terms = [term.strip() for term in content.split("/n") if term.strip()]
                else:
                    terms = [term.strip() for term in content.split("\n") if term.strip()]

            print(f"Loaded {len(terms)} terms from {file_path}")
            if terms:
                print(f"First 5 terms: {terms[:5]}")
            return terms
        except Exception as e:
            print(f"Error loading terms from {term_file}: {e}")
            return []

# ===============================
# EMBEDDING MODEL
# ===============================
class EmbeddingModel:
    """Handles embedding model loading and encoding."""

    def __init__(self, model_name: str = 'allenai/specter2', device: str = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None
        self.load_model(model_name)

    def load_model(self, model_name: str):
        """Load the embedding model."""
        print(f"Using device: {self.device}")

        try:
            if model_name == 'allenai/specter2':
                print("Loading SPECTER2 model...")
                from transformers import AutoTokenizer, AutoModel

                self.tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')
                self.model = AutoModel.from_pretrained('allenai/specter2_base').to(self.device)
                self.model.eval()
                print("SPECTER2 loaded successfully!")
            else:
                # Fallback to sentence transformer
                print(f"Loading model: {model_name}")
                self.model = SentenceTransformer(model_name).to(self.device)
                print(f"{model_name} loaded successfully!")

        except Exception as e:
            print(f"Error loading model: {e}")
            raise

    def encode(self, texts: List[str], batch_size: int = 256,
               convert_to_tensor: bool = True, **kwargs) -> torch.Tensor:
        """Encode texts to embeddings."""
        if not texts:
            raise ValueError("Cannot encode empty list of texts")

        if hasattr(self.model, 'encode') and callable(self.model.encode):
            # For SentenceTransformer models
            result = self.model.encode(
                texts,
                convert_to_tensor=convert_to_tensor,
                show_progress_bar=True,
                batch_size=batch_size,
                normalize_embeddings=True
            )
            if convert_to_tensor and not isinstance(result, torch.Tensor):
                result = torch.tensor(result, device=self.device)
            return result.to(self.device) if convert_to_tensor else result
        else:
            # For SPECTER2 models
            self.model.eval()
            all_embeddings = []

            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512
                ).to(self.device)

                with torch.no_grad():
                    outputs = self.model(**inputs)
                    embeddings = outputs.last_hidden_state[:, 0, :]
                    embeddings = F.normalize(embeddings, p=2, dim=1)
                    all_embeddings.append(embeddings.cpu())

            result = torch.cat(all_embeddings)
            return result.to(self.device) if convert_to_tensor else result

# ===============================
# SIMILARITY-BASED FILTERING
# ===============================
class SimilarityFilter:
    """Direct similarity-based filtering using SPECTER2 embeddings."""

    def __init__(self, domain_config: Dict, embedding_model: EmbeddingModel):
        self.domain_config = domain_config
        self.encoder = embedding_model
        self.domain_embeddings = None
        self.domain_centroid = None

    def encode_domain_terms(self, domain_terms: List[str]) -> torch.Tensor:
        """Encode domain terms with context augmentation."""
        print(f"Encoding {len(domain_terms)} domain terms...")
        context_prefix = self.domain_config['context_prefix']
        domain_terms_with_context = [f"{context_prefix}{term}" for term in domain_terms]
        embeddings = self.encoder.encode(domain_terms_with_context, batch_size=128)
        self.domain_embeddings = embeddings
        self.domain_centroid = embeddings.mean(dim=0, keepdim=True)
        return embeddings

    def compute_similarities(self, topics: List[str]) -> Dict[str, Any]:
        """Compute various similarity metrics for topics."""
        print(f"Encoding {len(topics)} topics...")

        # Encode topics with context
        topics_with_context = [f"topic: {topic}" for topic in topics]
        topic_embeddings = self.encoder.encode(
            topics_with_context,
            batch_size=512,
            convert_to_tensor=True
        )

        # Normalize embeddings
        topic_emb_norm = F.normalize(topic_embeddings, p=2, dim=1)
        domain_emb_norm = F.normalize(self.domain_embeddings, p=2, dim=1)

        # Compute similarity matrix
        print("Computing similarities...")
        similarity_matrix = torch.mm(topic_emb_norm, domain_emb_norm.T)

        # Compute various similarity metrics
        max_similarities, closest_indices = similarity_matrix.max(dim=1)
        avg_similarities = similarity_matrix.mean(dim=1)

        # Top-k similarities
        k = min(5, len(self.domain_embeddings))
        top_k_similarities, _ = similarity_matrix.topk(k=k, dim=1)
        avg_top_k = top_k_similarities.mean(dim=1)

        # Centroid similarity
        centroid_similarities = F.cosine_similarity(
            topic_emb_norm,
            self.domain_centroid
        )

        # Combined score
        combined_scores = (
            0.35 * max_similarities +
            0.25 * avg_similarities +
            0.20 * avg_top_k +
            0.20 * centroid_similarities
        )

        return {
            'topics': topics,
            'topic_embeddings': topic_embeddings,
            'similarity_matrix': similarity_matrix,
            'max_similarities': max_similarities,
            'avg_similarities': avg_similarities,
            'top_k_similarities': top_k_similarities,
            'avg_top_k': avg_top_k,
            'centroid_similarities': centroid_similarities,
            'combined_scores': combined_scores,
            'closest_indices': closest_indices
        }

    def filter_topics(self, all_topics: List[str],
                     domain_terms: List[str]) -> List[Dict]:
        """Filter topics using direct similarity metrics."""
        print(f"\n{'='*70}")
        print(f"Processing {len(all_topics)} topics for {self.domain_config['name']}")
        print('='*70)

        # Encode domain terms
        self.encode_domain_terms(domain_terms)

        # Process in chunks to manage memory
        chunk_size = 50000
        all_results = []

        for i in range(0, len(all_topics), chunk_size):
            chunk_topics = all_topics[i:i+chunk_size]
            print(f"Processing chunk {i//chunk_size + 1}/{(len(all_topics) + chunk_size - 1)//chunk_size}")

            # Compute similarities for chunk
            sim_results = self.compute_similarities(chunk_topics)

            # Create result dictionaries
            for j, topic in enumerate(chunk_topics):
                all_results.append({
                    'topic': topic,
                    'max_similarity': sim_results['max_similarities'][j].item(),
                    'avg_similarity': sim_results['avg_similarities'][j].item(),
                    'avg_top5_similarity': sim_results['avg_top_k'][j].item(),
                    'centroid_similarity': sim_results['centroid_similarities'][j].item(),
                    'combined_score': sim_results['combined_scores'][j].item(),
                    'closest_domain_term': domain_terms[sim_results['closest_indices'][j].item()],
                    'domain': self.domain_config['name']
                })

            # Memory cleanup
            del sim_results
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        # Sort by combined score
        all_results.sort(key=lambda x: x['combined_score'], reverse=True)

        # Adaptive thresholding
        combined_scores = np.array([r['combined_score'] for r in all_results])
        threshold = np.percentile(combined_scores, 85)

        # Filter by threshold and config limits
        config = self.domain_config
        filtered_results = []

        for result in all_results:
            if (result['combined_score'] >= threshold or
                len(filtered_results) < config['min_topics']):
                filtered_results.append(result)
            if len(filtered_results) >= config['max_topics']:
                break

        print(f"\nFiltering results:")
        print(f"  Total topics processed: {len(all_results)}")
        print(f"  Score threshold (85th percentile): {threshold:.3f}")
        print(f"  Topics after filtering: {len(filtered_results)}")
        print(f"  Top 5 topics:")
        for i, r in enumerate(filtered_results[:5]):
            print(f"    {i+1}. {r['topic']} (score: {r['combined_score']:.3f})")

        return filtered_results

# ===============================
# REPOSITORY FINDER
# ===============================
class RepositoryFinder:
    """Finds repositories related to specific domains."""

    def __init__(self, domain: str, filtered_topics: List[Dict]):
        self.domain = domain
        self.filtered_topics = filtered_topics
        self.strict_topic_set = self.create_strict_topic_set()
        self.topic_idf_scores = None

    def create_strict_topic_set(self) -> Set[str]:
        """Create strict set of domain topics based on similarity metrics."""
        strict_topics = set()
        config = DomainConfig.get_domain_config(self.domain)
        threshold = config.get('similarity_threshold', 0.6)

        for topic_data in self.filtered_topics:
            if topic_data['combined_score'] >= threshold:
                strict_topics.add(topic_data['topic'])

        print(f"Created strict topic set with {len(strict_topics)} topics")
        print(f"Threshold: {threshold}")
        return strict_topics

    def compute_idf_scores(self, filename: str = 'repos.ndjson',
                          n_workers: int = None) -> Dict[str, float]:
        """Compute IDF scores for all topics."""
        n_workers = n_workers or cpu_count()
        n_chunks = n_workers * 4

        print("Computing IDF scores...")

        # Read file in chunks
        chunks, total_lines = self._read_file_chunks(filename, n_chunks)

        # Count topics in parallel
        with Pool(n_workers) as pool:
            topic_counters = list(tqdm(
                pool.imap_unordered(self._count_topics_in_chunk, chunks),
                total=len(chunks),
                desc="Counting topics"
            ))

        # Merge counters and compute IDF
        all_topics_counter = Counter()
        for counter in topic_counters:
            all_topics_counter.update(counter)

        idf_scores = {}
        for topic, doc_freq in all_topics_counter.items():
            idf_scores[topic] = math.log(total_lines / doc_freq)

        self.topic_idf_scores = idf_scores
        print(f"Computed IDF for {len(idf_scores):,} unique topics")
        return idf_scores

    def find_repositories(self, filename: str = 'repos.ndjson',
                         n_workers: int = None) -> List[Dict]:
        """Find repositories containing domain topics."""
        n_workers = n_workers or cpu_count()
        n_chunks = n_workers * 4

        if self.topic_idf_scores is None:
            self.compute_idf_scores(filename, n_workers)

        print(f"\nFinding {self.domain} repositories...")

        # Read file in chunks
        chunks, total_lines = self._read_file_chunks(filename, n_chunks)

        # Process in parallel
        with Pool(n_workers, initializer=self._init_worker,
                  initargs=(self.topic_idf_scores, self.strict_topic_set)) as pool:
            results = list(tqdm(
                pool.imap_unordered(self._process_chunk, chunks),
                total=len(chunks),
                desc=f"Finding {self.domain} repos"
            ))

        # Merge results
        all_matching = []
        domain_topics_counter = Counter()

        for matching, counter in results:
            all_matching.extend(matching)
            domain_topics_counter.update(counter)

        # Analyze results
        print(f"\n{'='*60}")
        print(f"{self.domain.upper()} REPOSITORY FINDING RESULTS")
        print('='*60)
        print(f"Total matching repositories: {len(all_matching):,}")
        print(f"Percentage of total: {len(all_matching)/total_lines*100:.2f}%")

        print(f"\nTop 20 detected {self.domain} topics:")
        for topic, count in domain_topics_counter.most_common(20):
            print(f"  {topic}: {count:,}")

        return all_matching

    def save_results(self, matching_repos: List[Dict],
                    output_dir: str = "results"):
        """Save results to files."""
        os.makedirs(output_dir, exist_ok=True)

        # Save matching repositories
        output_file = os.path.join(output_dir, f"{self.domain}_repos.jsonl")
        with open(output_file, 'w', encoding='utf-8') as f:
            for repo in matching_repos:
                f.write(json.dumps(repo) + '\n')

        # Save filtered topics
        topics_file = os.path.join(output_dir, f"{self.domain}_filtered_topics.json")
        with open(topics_file, 'w') as f:
            json.dump(self.filtered_topics, f, indent=2)

        # Save strict topics
        strict_topics_file = os.path.join(output_dir, f"{self.domain}_topics.txt")
        with open(strict_topics_file, 'w') as f:
            for topic in sorted(self.strict_topic_set):
                f.write(f"{topic}\n")

        print(f"\nSaved results to {output_dir}:")
        print(f"  - {output_file}")
        print(f"  - {topics_file}")
        print(f"  - {strict_topics_file}")

    # Helper methods for parallel processing
    @staticmethod
    def _read_file_chunks(filename: str, num_chunks: int) -> Tuple[List[List[str]], int]:
        """Read file and split into chunks."""
        print("Loading file into memory...")
        with open(filename, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()

        total_lines = len(all_lines)
        chunk_size = total_lines // num_chunks + 1

        chunks = []
        for i in range(0, total_lines, chunk_size):
            chunks.append(all_lines[i:i + chunk_size])

        return chunks, total_lines

    @staticmethod
    def _count_topics_in_chunk(lines: List[str]) -> Counter:
        """Count topics in a chunk."""
        counter = Counter()
        for line in lines:
            try:
                data = json.loads(line.strip())
                topics = data.get('topics', [])
                if topics:
                    counter.update(topics)
            except:
                pass
        return counter

    @staticmethod
    def _process_chunk(lines: List[str]) -> Tuple[List[Dict], Counter]:
        """Process chunk to find domain repositories."""
        matching = []
        topic_counter = Counter()

        global _worker_idf_scores, _worker_topic_set

        for line in lines:
            try:
                data = json.loads(line.strip())
                topics = data.get('topics', [])

                if not topics:
                    continue

                # Get only domain topics
                domain_topics = [t for t in topics if t in _worker_topic_set]

                if not domain_topics:
                    continue

                # Calculate scores
                domain_score = sum(_worker_idf_scores.get(t, 0) for t in domain_topics)
                total_score = sum(_worker_idf_scores.get(t, 0) for t in topics)

                if total_score > 0:
                    domain_ratio = domain_score / total_score

                    # Adaptive threshold
                    num_topics = len(topics)
                    if num_topics <= 3:
                        threshold = 0.4
                    elif num_topics <= 6:
                        threshold = 0.35
                    else:
                        threshold = 0.3

                    if domain_ratio >= threshold:
                        matching.append({
                            'swhid': data.get('swhid'),
                            'topics': topics,
                            'url': data.get('url', []),
                            'domain_topics': domain_topics,
                            'domain_ratio': round(domain_ratio, 3),
                            'num_topics': num_topics,
                            'domain': 'unknown'  # Placeholder
                        })
                        topic_counter.update(domain_topics)
            except:
                pass

        return matching, topic_counter

    @staticmethod
    def _init_worker(idf_dict: Dict, topic_set: Set):
        """Initialize worker process."""
        global _worker_idf_scores, _worker_topic_set
        _worker_idf_scores = idf_dict
        _worker_topic_set = topic_set

# ===============================
# BLOB COLLECTOR
# ===============================
class BlobCollector:
    """Collects blob files for matching repositories."""

    @staticmethod
    def build_blob_paths(matching_repos: List[Dict],
                        extraction_dir: str = "extracted_data") -> List[str]:
        """Build blob file paths from matching repositories."""
        blob_paths = []

        for repo in matching_repos:
            readme = repo.get('readme')
            if readme:
                # Extract hash from readme (format: swh:1:cnt:hash)
                hash_part = readme.split(':')[-1]

                if hash_part and len(hash_part) >= 4:
                    first_two = hash_part[:2]
                    next_two = hash_part[2:4]

                    blob_pattern = f"{extraction_dir}/blobs-by-swhid/{first_two}/{next_two}/{hash_part}"
                    blob_paths.append(blob_pattern)

        print(f"Built {len(blob_paths)} blob paths")
        return blob_paths

    @staticmethod
    def collect_files(blob_paths: List[str], output_dir: str = "collected_blobs"):
        """Collect files into directory."""
        os.makedirs(output_dir, exist_ok=True)
        collected_count = 0

        for blob_path in blob_paths:
            if os.path.exists(blob_path):
                # Copy file to output directory
                import shutil
                shutil.copy2(blob_path, output_dir)
                collected_count += 1

        print(f"Collected {collected_count} files to {output_dir}")
        return collected_count

# ===============================
# MAIN PIPELINE
# ===============================
class MultiDomainPipeline:
    """Main pipeline for processing multiple scientific domains."""

    def __init__(self, domains: List[str] = None, data_dir: str = "data"):
        if domains is None:
            domains = ['math', 'physics', 'biology']
        self.domains = domains
        self.data_dir = data_dir
        self.results = {}
        os.makedirs(data_dir, exist_ok=True)

    def run(self):
        """Run the complete pipeline for all domains."""
        print("="*70)
        print("MULTI-DOMAIN SOFTWARE REPOSITORY PIPELINE")
        print("="*70)

        # Step 1: Load all topics from GitHub data
        print("\n[STEP 1] Loading GitHub topics...")
        repos_file = os.path.join(self.data_dir, "repos.ndjson")
        if not os.path.exists(repos_file):
            print(f"Error: GitHub topics file not found at {repos_file}")
            print("Please ensure 'repos.ndjson' is in the data directory.")
            return

        all_topics, topic_counter = self._load_all_topics(repos_file)

        # Step 2: Initialize embedding model
        print("\n[STEP 2] Initializing embedding model...")
        embedding_model = EmbeddingModel('allenai/specter2')

        # Step 3: Process each domain
        for domain in self.domains:
            print(f"\n{'='*70}")
            print(f"PROCESSING DOMAIN: {domain.upper()}")
            print('='*70)

            try:
                domain_results = self._process_domain(
                    domain, all_topics, embedding_model
                )
                self.results[domain] = domain_results

                print(f"\nâœ… Completed {domain.upper()} processing!")
                print(f"   Found {len(domain_results['filtered_topics'])} domain topics")
                print(f"   Found {len(domain_results['matching_repos'])} repositories")

            except Exception as e:
                print(f"âŒ Error processing {domain}: {e}")
                import traceback
                traceback.print_exc()
                continue

        # Step 4: Generate combined report
        self._generate_report()

        # Step 5: Collect blob files for each domain (optional)
        print("\n[STEP 5] Collecting blob files...")
        extraction_dir = os.path.join(self.data_dir, "extracted_data")
        if os.path.exists(extraction_dir):
            for domain, results in self.results.items():
                if results and 'matching_repos' in results:
                    blob_paths = BlobCollector.build_blob_paths(
                        results['matching_repos'], extraction_dir
                    )
                    output_dir = os.path.join(self.data_dir, f"{domain}_blobs")
                    collected = BlobCollector.collect_files(blob_paths, output_dir)
                    print(f"  {domain}: collected {collected} blobs")
        else:
            print("Blob extraction directory not found. Skipping blob collection.")

        print("\n" + "="*70)
        print("PIPELINE COMPLETE!")
        print("="*70)

    def _load_all_topics(self, repos_file: str) -> Tuple[List[str], Counter]:
        """Load all topics from the GitHub dataset."""
        topic_counter = Counter()

        with open(repos_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="Loading topics"):
                try:
                    data = json.loads(line.strip())
                    topics = data.get('topics', [])
                    if topics:
                        topic_counter.update(topics)
                except:
                    pass

        all_topics = list(topic_counter.keys())
        print(f"Loaded {len(all_topics)} unique topics")
        print(f"Total topic occurrences: {sum(topic_counter.values()):,}")
        return all_topics, topic_counter

    def _process_domain(self, domain: str, all_topics: List[str],
                       embedding_model: EmbeddingModel) -> Dict:
        """Process a single domain."""
        config = DomainConfig.get_domain_config(domain)

        # Load domain terms
        term_loader = DomainTermLoader()
        domain_terms = term_loader.load_terms(config['term_file'])

        if not domain_terms:
            print(f"Warning: No terms loaded for {domain}")
            return {}

        # Apply similarity filtering
        similarity_filter = SimilarityFilter(config, embedding_model)
        filtered_topics = similarity_filter.filter_topics(all_topics, domain_terms)

        # Find repositories
        repo_finder = RepositoryFinder(domain, filtered_topics)
        repos_file = os.path.join(self.data_dir, "repos.ndjson")
        matching_repos = repo_finder.find_repositories(repos_file)

        # Save results
        results_dir = os.path.join(self.data_dir, "results", domain)
        repo_finder.save_results(matching_repos, results_dir)

        return {
            'config': config,
            'domain_terms': domain_terms,
            'filtered_topics': filtered_topics,
            'matching_repos': matching_repos
        }

    def _generate_report(self):
        """Generate a comprehensive report of all domains."""
        print("\n" + "="*70)
        print("COMBINED RESULTS REPORT")
        print("="*70)

        total_repos = 0
        for domain, results in self.results.items():
            if not results:
                continue

            filtered_topics = results.get('filtered_topics', [])
            matching_repos = results.get('matching_repos', [])
            total_repos += len(matching_repos)

            print(f"\n{domain.upper()}:")
            print(f"  - Domain terms: {len(results.get('domain_terms', []))}")
            print(f"  - Filtered topics: {len(filtered_topics)}")
            print(f"  - Matching repositories: {len(matching_repos)}")

            if filtered_topics:
                print(f"  - Top 5 topics:")
                for i, topic in enumerate(filtered_topics[:5]):
                    print(f"    {i+1}. {topic['topic']} (score: {topic.get('combined_score', 0):.3f})")

        print(f"\nTotal repositories found across all domains: {total_repos:,}")

# ===============================
# EXECUTION
# ===============================
if __name__ == "__main__":
    # Install required packages if not available
    try:
        import transformers
        import sentence_transformers
        print("Required packages are available.")
    except ImportError:
        print("Some packages may need to be installed:")
        print("pip install transformers sentence-transformers torch")

    # Configure which domains to process
    domains_to_process = ['math', 'physics', 'biology']  # Adjust as needed

    # Create and run pipeline
    pipeline = MultiDomainPipeline(domains=domains_to_process, data_dir=".")
    pipeline.run()

# ===============================
# MINIMAL INFOMANIAK UPLOADER
# ===============================
import requests
import os
from pathlib import Path


def upload_to_infomaniak():
    """Upload all result files to Infomaniak using v3 API."""

    # YOUR CREDENTIALS - Use the same as in your working code
    TOKEN = "Fi9Aaadhibox6l50TcideMovSUo8JtaKc30i_yP-XJFcf_db9HL_BX9KQ0HZN41H8u53Pu6tGRq3PGx5"
    DRIVE_ID = "705884"
    FOLDER_ID = "20521"  # Your upload folder ID

    # Your prefix
    FILE_PREFIX = "SPECTER2_filtering_"

    # Results directories
    RESULTS_DIR = "results"
    DOMAINS = ['math', 'physics', 'biology']

    # API endpoint - MATCHING YOUR WORKING CODE
    UPLOAD_URL = f"https://api.infomaniak.com/3/drive/{DRIVE_ID}/upload"

    print(f"\nUploading files from {RESULTS_DIR}/ to Infomaniak Drive...")

    for domain in DOMAINS:
        domain_dir = os.path.join(RESULTS_DIR, domain)

        if not os.path.exists(domain_dir):
            print(f"Directory not found: {domain_dir}")
            continue

        print(f"\n{domain.upper()} files:")

        for filename in os.listdir(domain_dir):
            filepath = os.path.join(domain_dir, filename)

            if os.path.isfile(filepath):
                try:
                    # Add your prefix
                    upload_filename = f"{FILE_PREFIX}{filename}"

                    # Get file size
                    file_size = os.path.getsize(filepath)
                    print(f"  ðŸ“¤ {upload_filename} ({file_size:,} bytes)...")

                    # Read file content
                    with open(filepath, 'rb') as f:
                        file_content = f.read()

                    # Prepare parameters - MATCHING YOUR WORKING CODE
                    params = {
                        "total_size": len(file_content),
                        "directory_id": FOLDER_ID,
                        "file_name": upload_filename
                    }

                    headers = {
                        "Authorization": f"Bearer {TOKEN}",
                        # "Content-Type": "application/octet-stream"  # NOT needed for multipart
                    }

                    # Make the request - passing params and data separately
                    response = requests.post(
                        UPLOAD_URL,
                        headers=headers,
                        params=params,  # parameters go here
                        data=file_content  # file data goes here
                    )

                    if response.status_code == 200:
                        result = response.json()
                        if result.get('result') == 'success':
                            print(f"    âœ“ Uploaded successfully")
                        else:
                            print(f"    âœ— Failed: {result.get('error', 'Unknown error')}")
                    else:
                        print(f"    âœ— HTTP {response.status_code}: {response.text[:100]}")

                except Exception as e:
                    print(f"    âœ— Error: {str(e)[:80]}")

    print("\nUpload complete!")


# ===============================
# EVEN SIMPLER VERSION
# ===============================
def simple_upload():
    """Simplest version using the exact same pattern as your working code."""

    TOKEN = "Fi9Aaadhibox6l50TcideMovSUo8JtaKc30i_yP-XJFcf_db9HL_BX9KQ0HZN41H8u53Pu6tGRq3PGx5"
    DRIVE_ID = "705884"
    FOLDER_ID = "20521"
    PREFIX = "SPECTER2_filtering_"

    UPLOAD_URL = f"https://api.infomaniak.com/3/drive/{DRIVE_ID}/upload"

    print("Uploading result files...")

    # Find all result files
    for root, dirs, files in os.walk("results"):
        for file in files:
            if file.endswith(('.jsonl', '.json', '.txt')):
                filepath = os.path.join(root, file)
                domain = os.path.basename(root)
                upload_filename = f"{PREFIX}{domain}_{file}"

                print(f"Uploading: {upload_filename}")

                try:
                    with open(filepath, 'rb') as f:
                        content = f.read()

                    response = requests.post(
                        UPLOAD_URL,
                        headers={"Authorization": f"Bearer {TOKEN}"},
                        params={
                            "total_size": len(content),
                            "directory_id": FOLDER_ID,
                            "file_name": upload_filename
                        },
                        data=content
                    )

                    if response.ok:
                        print(f"  âœ“ Success")
                    else:
                        print(f"  âœ— Failed: {response.status_code}")

                except Exception as e:
                    print(f"  âœ— Error: {e}")


# ===============================
# MAIN EXECUTION
# ===============================
if __name__ == "__main__":
    # Upload results
    if os.path.exists("results"):
        print("Found results directory, uploading...")
        simple_upload()  # Use the simpler version
    else:
        print("Results directory not found.")

# -*- coding: utf-8 -*-
"""MultiDomainSoftwareReposFinder.ipynb

Automatically generated pipeline for finding mathematical, physics, and biology software repositories.
"""
# ===============================
# BASELINE 2
# ===============================
# ===============================
# IMPORTS & SETUP
# ===============================
import os
import json
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
import math
import warnings
import tarfile
import zstandard as zstd
from pathlib import Path
from typing import List, Dict, Tuple, Set, Any, Optional
from collections import Counter
from tqdm.auto import tqdm
from multiprocessing import Pool, cpu_count
from sentence_transformers import SentenceTransformer
warnings.filterwarnings('ignore')

# ===============================
# CONFIGURATION
# ===============================
class DomainConfig:
    """Configuration for different scientific domains."""

    DOMAINS = {
        'math': {
            'name': 'Mathematics',
            'term_file': 'mathematics_keywords.txt',
            'context_prefix': 'mathematical concept: ',
            'similarity_threshold': 0.6,
            'min_topics': 1000,
            'max_topics': 5000
        },
        'physics': {
            'name': 'Physics',
            'term_file': 'physics_keywords.txt',
            'context_prefix': 'physics concept: ',
            'similarity_threshold': 0.6,
            'min_topics': 1000,
            'max_topics': 5000
        },
        'biology': {
            'name': 'Biology',
            'term_file': 'biology_keywords.txt',
            'context_prefix': 'biological concept: ',
            'similarity_threshold': 0.6,
            'min_topics': 1000,
            'max_topics': 5000
        }
    }

    @classmethod
    def get_domain_config(cls, domain: str) -> Dict:
        """Get configuration for a specific domain."""
        if domain not in cls.DOMAINS:
            raise ValueError(f"Domain must be one of: {list(cls.DOMAINS.keys())}")
        return cls.DOMAINS[domain]

# ===============================
# DOMAIN TERM LOADER
# ===============================
class DomainTermLoader:
    """Loads and processes domain-specific terms."""

    @staticmethod
    def load_terms(term_file: str) -> List[str]:
        """Load domain terms from file."""
        try:
            # Try multiple possible locations
            possible_paths = [
                Path(term_file),
                Path("downloaded_files") / term_file,
                Path("data") / term_file,
                Path(".") / term_file
            ]

            file_path = None
            for path in possible_paths:
                if path.exists():
                    file_path = path
                    break

            if file_path is None:
                print(f"Warning: Term file {term_file} not found in any expected location.")
                return []

            with open(file_path, "r", encoding='utf-8') as f:
                content = f.read()
                # Handle different separators
                if "/n" in content:
                    terms = [term.strip() for term in content.split("/n") if term.strip()]
                else:
                    terms = [term.strip() for term in content.split("\n") if term.strip()]

            print(f"Loaded {len(terms)} terms from {file_path}")
            if terms:
                print(f"First 5 terms: {terms[:5]}")
            return terms
        except Exception as e:
            print(f"Error loading terms from {term_file}: {e}")
            return []

# ===============================
# EMBEDDING MODEL
# ===============================
class EmbeddingModel:
    """Handles embedding model loading and encoding."""

    def __init__(self, model_name: str = 'allenai/scibert_scivocab_uncased', device: str = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None
        self.load_model(model_name)

    def load_model(self, model_name: str):
        """Load the embedding model."""
        print(f"Using device: {self.device}")

        try:
            if model_name == 'allenai/scibert_scivocab_uncased':
                print("Loading SCIBERT model...")
                from transformers import AutoTokenizer, AutoModel

                self.tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')
                self.model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased').to(self.device)
                self.model.eval()
                print("SCIBERT loaded successfully!")
            else:
                # Fallback to sentence transformer
                print(f"Loading model: {model_name}")
                self.model = SentenceTransformer(model_name).to(self.device)
                print(f"{model_name} loaded successfully!")

        except Exception as e:
            print(f"Error loading model: {e}")
    def encode(self, texts: List[str], batch_size: int = 256,
               convert_to_tensor: bool = True, **kwargs) -> torch.Tensor:
        """Encode texts to embeddings."""
        if not texts:
            raise ValueError("Cannot encode empty list of texts")

        if hasattr(self.model, 'encode') and callable(self.model.encode):
            # For SentenceTransformer models
            result = self.model.encode(
                texts,
                convert_to_tensor=convert_to_tensor,
                show_progress_bar=True,
                batch_size=batch_size,
                normalize_embeddings=True
            )
            if convert_to_tensor and not isinstance(result, torch.Tensor):
                result = torch.tensor(result, device=self.device)
            return result.to(self.device) if convert_to_tensor else result
        else:
            # For SPECTER2 models
            self.model.eval()
            all_embeddings = []

            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512
                ).to(self.device)

                with torch.no_grad():
                    outputs = self.model(**inputs)
                    embeddings = outputs.last_hidden_state[:, 0, :]
                    embeddings = F.normalize(embeddings, p=2, dim=1)
                    all_embeddings.append(embeddings.cpu())

            result = torch.cat(all_embeddings)
            return result.to(self.device) if convert_to_tensor else result

# ===============================
# SIMILARITY-BASED FILTERING
# ===============================
class SimilarityFilter:
    """Direct similarity-based filtering using SPECTER2 embeddings."""

    def __init__(self, domain_config: Dict, embedding_model: EmbeddingModel):
        self.domain_config = domain_config
        self.encoder = embedding_model
        self.domain_embeddings = None
        self.domain_centroid = None

    def encode_domain_terms(self, domain_terms: List[str]) -> torch.Tensor:
        """Encode domain terms with context augmentation."""
        print(f"Encoding {len(domain_terms)} domain terms...")
        context_prefix = self.domain_config['context_prefix']
        domain_terms_with_context = [f"{context_prefix}{term}" for term in domain_terms]
        embeddings = self.encoder.encode(domain_terms_with_context, batch_size=128)
        self.domain_embeddings = embeddings
        self.domain_centroid = embeddings.mean(dim=0, keepdim=True)
        return embeddings

    def compute_similarities(self, topics: List[str]) -> Dict[str, Any]:
        """Compute various similarity metrics for topics."""
        print(f"Encoding {len(topics)} topics...")

        # Encode topics with context
        topics_with_context = [f"topic: {topic}" for topic in topics]
        topic_embeddings = self.encoder.encode(
            topics_with_context,
            batch_size=512,
            convert_to_tensor=True
        )

        # Normalize embeddings
        topic_emb_norm = F.normalize(topic_embeddings, p=2, dim=1)
        domain_emb_norm = F.normalize(self.domain_embeddings, p=2, dim=1)

        # Compute similarity matrix
        print("Computing similarities...")
        similarity_matrix = torch.mm(topic_emb_norm, domain_emb_norm.T)

        # Compute various similarity metrics
        max_similarities, closest_indices = similarity_matrix.max(dim=1)
        avg_similarities = similarity_matrix.mean(dim=1)

        # Top-k similarities
        k = min(5, len(self.domain_embeddings))
        top_k_similarities, _ = similarity_matrix.topk(k=k, dim=1)
        avg_top_k = top_k_similarities.mean(dim=1)

        # Centroid similarity
        centroid_similarities = F.cosine_similarity(
            topic_emb_norm,
            self.domain_centroid
        )

        # Combined score
        combined_scores = (
            0.35 * max_similarities +
            0.25 * avg_similarities +
            0.20 * avg_top_k +
            0.20 * centroid_similarities
        )

        return {
            'topics': topics,
            'topic_embeddings': topic_embeddings,
            'similarity_matrix': similarity_matrix,
            'max_similarities': max_similarities,
            'avg_similarities': avg_similarities,
            'top_k_similarities': top_k_similarities,
            'avg_top_k': avg_top_k,
            'centroid_similarities': centroid_similarities,
            'combined_scores': combined_scores,
            'closest_indices': closest_indices
        }

    def filter_topics(self, all_topics: List[str],
                     domain_terms: List[str]) -> List[Dict]:
        """Filter topics using direct similarity metrics."""
        print(f"\n{'='*70}")
        print(f"Processing {len(all_topics)} topics for {self.domain_config['name']}")
        print('='*70)

        # Encode domain terms
        self.encode_domain_terms(domain_terms)

        # Process in chunks to manage memory
        chunk_size = 50000
        all_results = []

        for i in range(0, len(all_topics), chunk_size):
            chunk_topics = all_topics[i:i+chunk_size]
            print(f"Processing chunk {i//chunk_size + 1}/{(len(all_topics) + chunk_size - 1)//chunk_size}")

            # Compute similarities for chunk
            sim_results = self.compute_similarities(chunk_topics)

            # Create result dictionaries
            for j, topic in enumerate(chunk_topics):
                all_results.append({
                    'topic': topic,
                    'max_similarity': sim_results['max_similarities'][j].item(),
                    'avg_similarity': sim_results['avg_similarities'][j].item(),
                    'avg_top5_similarity': sim_results['avg_top_k'][j].item(),
                    'centroid_similarity': sim_results['centroid_similarities'][j].item(),
                    'combined_score': sim_results['combined_scores'][j].item(),
                    'closest_domain_term': domain_terms[sim_results['closest_indices'][j].item()],
                    'domain': self.domain_config['name']
                })

            # Memory cleanup
            del sim_results
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        # Sort by combined score
        all_results.sort(key=lambda x: x['combined_score'], reverse=True)

        # Adaptive thresholding
        combined_scores = np.array([r['combined_score'] for r in all_results])
        threshold = np.percentile(combined_scores, 85)

        # Filter by threshold and config limits
        config = self.domain_config
        filtered_results = []

        for result in all_results:
            if (result['combined_score'] >= threshold or
                len(filtered_results) < config['min_topics']):
                filtered_results.append(result)
            if len(filtered_results) >= config['max_topics']:
                break

        print(f"\nFiltering results:")
        print(f"  Total topics processed: {len(all_results)}")
        print(f"  Score threshold (85th percentile): {threshold:.3f}")
        print(f"  Topics after filtering: {len(filtered_results)}")
        print(f"  Top 5 topics:")
        for i, r in enumerate(filtered_results[:5]):
            print(f"    {i+1}. {r['topic']} (score: {r['combined_score']:.3f})")

        return filtered_results

# ===============================
# REPOSITORY FINDER
# ===============================
class RepositoryFinder:
    """Finds repositories related to specific domains."""

    def __init__(self, domain: str, filtered_topics: List[Dict]):
        self.domain = domain
        self.filtered_topics = filtered_topics
        self.strict_topic_set = self.create_strict_topic_set()
        self.topic_idf_scores = None

    def create_strict_topic_set(self) -> Set[str]:
        """Create strict set of domain topics based on similarity metrics."""
        strict_topics = set()
        config = DomainConfig.get_domain_config(self.domain)
        threshold = config.get('similarity_threshold', 0.6)

        for topic_data in self.filtered_topics:
            if topic_data['combined_score'] >= threshold:
                strict_topics.add(topic_data['topic'])

        print(f"Created strict topic set with {len(strict_topics)} topics")
        print(f"Threshold: {threshold}")
        return strict_topics

    def compute_idf_scores(self, filename: str = 'repos.ndjson',
                          n_workers: int = None) -> Dict[str, float]:
        """Compute IDF scores for all topics."""
        n_workers = n_workers or cpu_count()
        n_chunks = n_workers * 4

        print("Computing IDF scores...")

        # Read file in chunks
        chunks, total_lines = self._read_file_chunks(filename, n_chunks)

        # Count topics in parallel
        with Pool(n_workers) as pool:
            topic_counters = list(tqdm(
                pool.imap_unordered(self._count_topics_in_chunk, chunks),
                total=len(chunks),
                desc="Counting topics"
            ))

        # Merge counters and compute IDF
        all_topics_counter = Counter()
        for counter in topic_counters:
            all_topics_counter.update(counter)

        idf_scores = {}
        for topic, doc_freq in all_topics_counter.items():
            idf_scores[topic] = math.log(total_lines / doc_freq)

        self.topic_idf_scores = idf_scores
        print(f"Computed IDF for {len(idf_scores):,} unique topics")
        return idf_scores

    def find_repositories(self, filename: str = 'repos.ndjson',
                         n_workers: int = None) -> List[Dict]:
        """Find repositories containing domain topics."""
        n_workers = n_workers or cpu_count()
        n_chunks = n_workers * 4

        if self.topic_idf_scores is None:
            self.compute_idf_scores(filename, n_workers)

        print(f"\nFinding {self.domain} repositories...")

        # Read file in chunks
        chunks, total_lines = self._read_file_chunks(filename, n_chunks)

        # Process in parallel
        with Pool(n_workers, initializer=self._init_worker,
                  initargs=(self.topic_idf_scores, self.strict_topic_set)) as pool:
            results = list(tqdm(
                pool.imap_unordered(self._process_chunk, chunks),
                total=len(chunks),
                desc=f"Finding {self.domain} repos"
            ))

        # Merge results
        all_matching = []
        domain_topics_counter = Counter()

        for matching, counter in results:
            all_matching.extend(matching)
            domain_topics_counter.update(counter)

        # Analyze results
        print(f"\n{'='*60}")
        print(f"{self.domain.upper()} REPOSITORY FINDING RESULTS")
        print('='*60)
        print(f"Total matching repositories: {len(all_matching):,}")
        print(f"Percentage of total: {len(all_matching)/total_lines*100:.2f}%")

        print(f"\nTop 20 detected {self.domain} topics:")
        for topic, count in domain_topics_counter.most_common(20):
            print(f"  {topic}: {count:,}")

        return all_matching

    def save_results(self, matching_repos: List[Dict],
                    output_dir: str = "results"):
        """Save results to files."""
        os.makedirs(output_dir, exist_ok=True)

        # Save matching repositories
        output_file = os.path.join(output_dir, f"{self.domain}_repos.jsonl")
        with open(output_file, 'w', encoding='utf-8') as f:
            for repo in matching_repos:
                f.write(json.dumps(repo) + '\n')

        # Save filtered topics
        topics_file = os.path.join(output_dir, f"{self.domain}_filtered_topics.json")
        with open(topics_file, 'w') as f:
            json.dump(self.filtered_topics, f, indent=2)

        # Save strict topics
        strict_topics_file = os.path.join(output_dir, f"{self.domain}_topics.txt")
        with open(strict_topics_file, 'w') as f:
            for topic in sorted(self.strict_topic_set):
                f.write(f"{topic}\n")

        print(f"\nSaved results to {output_dir}:")
        print(f"  - {output_file}")
        print(f"  - {topics_file}")
        print(f"  - {strict_topics_file}")

    # Helper methods for parallel processing
    @staticmethod
    def _read_file_chunks(filename: str, num_chunks: int) -> Tuple[List[List[str]], int]:
        """Read file and split into chunks."""
        print("Loading file into memory...")
        with open(filename, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()

        total_lines = len(all_lines)
        chunk_size = total_lines // num_chunks + 1

        chunks = []
        for i in range(0, total_lines, chunk_size):
            chunks.append(all_lines[i:i + chunk_size])

        return chunks, total_lines

    @staticmethod
    def _count_topics_in_chunk(lines: List[str]) -> Counter:
        """Count topics in a chunk."""
        counter = Counter()
        for line in lines:
            try:
                data = json.loads(line.strip())
                topics = data.get('topics', [])
                if topics:
                    counter.update(topics)
            except:
                pass
        return counter

    @staticmethod
    def _process_chunk(lines: List[str]) -> Tuple[List[Dict], Counter]:
        """Process chunk to find domain repositories."""
        matching = []
        topic_counter = Counter()

        global _worker_idf_scores, _worker_topic_set

        for line in lines:
            try:
                data = json.loads(line.strip())
                topics = data.get('topics', [])

                if not topics:
                    continue

                # Get only domain topics
                domain_topics = [t for t in topics if t in _worker_topic_set]

                if not domain_topics:
                    continue

                # Calculate scores
                domain_score = sum(_worker_idf_scores.get(t, 0) for t in domain_topics)
                total_score = sum(_worker_idf_scores.get(t, 0) for t in topics)

                if total_score > 0:
                    domain_ratio = domain_score / total_score

                    # Adaptive threshold
                    num_topics = len(topics)
                    if num_topics <= 3:
                        threshold = 0.4
                    elif num_topics <= 6:
                        threshold = 0.35
                    else:
                        threshold = 0.3

                    if domain_ratio >= threshold:
                        matching.append({
                            'swhid': data.get('swhid'),
                            'topics': topics,
                            'url': data.get('url', []),
                            'domain_topics': domain_topics,
                            'domain_ratio': round(domain_ratio, 3),
                            'num_topics': num_topics,
                            'domain': 'unknown'  # Placeholder
                        })
                        topic_counter.update(domain_topics)
            except:
                pass

        return matching, topic_counter

    @staticmethod
    def _init_worker(idf_dict: Dict, topic_set: Set):
        """Initialize worker process."""
        global _worker_idf_scores, _worker_topic_set
        _worker_idf_scores = idf_dict
        _worker_topic_set = topic_set

# ===============================
# BLOB COLLECTOR
# ===============================
class BlobCollector:
    """Collects blob files for matching repositories."""

    @staticmethod
    def build_blob_paths(matching_repos: List[Dict],
                        extraction_dir: str = "extracted_data") -> List[str]:
        """Build blob file paths from matching repositories."""
        blob_paths = []

        for repo in matching_repos:
            readme = repo.get('readme')
            if readme:
                # Extract hash from readme (format: swh:1:cnt:hash)
                hash_part = readme.split(':')[-1]

                if hash_part and len(hash_part) >= 4:
                    first_two = hash_part[:2]
                    next_two = hash_part[2:4]

                    blob_pattern = f"{extraction_dir}/blobs-by-swhid/{first_two}/{next_two}/{hash_part}"
                    blob_paths.append(blob_pattern)

        print(f"Built {len(blob_paths)} blob paths")
        return blob_paths

    @staticmethod
    def collect_files(blob_paths: List[str], output_dir: str = "collected_blobs"):
        """Collect files into directory."""
        os.makedirs(output_dir, exist_ok=True)
        collected_count = 0

        for blob_path in blob_paths:
            if os.path.exists(blob_path):
                # Copy file to output directory
                import shutil
                shutil.copy2(blob_path, output_dir)
                collected_count += 1

        print(f"Collected {collected_count} files to {output_dir}")
        return collected_count

# ===============================
# MAIN PIPELINE
# ===============================
class MultiDomainPipeline:
    """Main pipeline for processing multiple scientific domains."""

    def __init__(self, domains: List[str] = None, data_dir: str = "data"):
        if domains is None:
            domains = ['math', 'physics', 'biology']
        self.domains = domains
        self.data_dir = data_dir
        self.results = {}
        os.makedirs(data_dir, exist_ok=True)

    def run(self):
        """Run the complete pipeline for all domains."""
        print("="*70)
        print("MULTI-DOMAIN SOFTWARE REPOSITORY PIPELINE")
        print("="*70)

        # Step 1: Load all topics from GitHub data
        print("\n[STEP 1] Loading GitHub topics...")
        repos_file = os.path.join(self.data_dir, "repos.ndjson")
        if not os.path.exists(repos_file):
            print(f"Error: GitHub topics file not found at {repos_file}")
            print("Please ensure 'repos.ndjson' is in the data directory.")
            return

        all_topics, topic_counter = self._load_all_topics(repos_file)

        # Step 2: Initialize embedding model
        print("\n[STEP 2] Initializing embedding model...")
        embedding_model = EmbeddingModel('allenai/scibert_scivocab_uncased')

        # Step 3: Process each domain
        for domain in self.domains:
            print(f"\n{'='*70}")
            print(f"PROCESSING DOMAIN: {domain.upper()}")
            print('='*70)

            try:
                domain_results = self._process_domain(
                    domain, all_topics, embedding_model
                )
                self.results[domain] = domain_results

                print(f"\nâœ… Completed {domain.upper()} processing!")
                print(f"   Found {len(domain_results['filtered_topics'])} domain topics")
                print(f"   Found {len(domain_results['matching_repos'])} repositories")

            except Exception as e:
                print(f"âŒ Error processing {domain}: {e}")
                import traceback
                traceback.print_exc()
                continue

        # Step 4: Generate combined report
        self._generate_report()

        # Step 5: Collect blob files for each domain (optional)
        print("\n[STEP 5] Collecting blob files...")
        extraction_dir = os.path.join(self.data_dir, "extracted_data")
        if os.path.exists(extraction_dir):
            for domain, results in self.results.items():
                if results and 'matching_repos' in results:
                    blob_paths = BlobCollector.build_blob_paths(
                        results['matching_repos'], extraction_dir
                    )
                    output_dir = os.path.join(self.data_dir, f"{domain}_blobs")
                    collected = BlobCollector.collect_files(blob_paths, output_dir)
                    print(f"  {domain}: collected {collected} blobs")
        else:
            print("Blob extraction directory not found. Skipping blob collection.")

        print("\n" + "="*70)
        print("PIPELINE COMPLETE!")
        print("="*70)

    def _load_all_topics(self, repos_file: str) -> Tuple[List[str], Counter]:
        """Load all topics from the GitHub dataset."""
        topic_counter = Counter()

        with open(repos_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="Loading topics"):
                try:
                    data = json.loads(line.strip())
                    topics = data.get('topics', [])
                    if topics:
                        topic_counter.update(topics)
                except:
                    pass

        all_topics = list(topic_counter.keys())
        print(f"Loaded {len(all_topics)} unique topics")
        print(f"Total topic occurrences: {sum(topic_counter.values()):,}")
        return all_topics, topic_counter

    def _process_domain(self, domain: str, all_topics: List[str],
                       embedding_model: EmbeddingModel) -> Dict:
        """Process a single domain."""
        config = DomainConfig.get_domain_config(domain)

        # Load domain terms
        term_loader = DomainTermLoader()
        domain_terms = term_loader.load_terms(config['term_file'])

        if not domain_terms:
            print(f"Warning: No terms loaded for {domain}")
            return {}

        # Apply similarity filtering
        similarity_filter = SimilarityFilter(config, embedding_model)
        filtered_topics = similarity_filter.filter_topics(all_topics, domain_terms)

        # Find repositories
        repo_finder = RepositoryFinder(domain, filtered_topics)
        repos_file = os.path.join(self.data_dir, "repos.ndjson")
        matching_repos = repo_finder.find_repositories(repos_file)

        # Save results
        results_dir = os.path.join(self.data_dir, "results", domain)
        repo_finder.save_results(matching_repos, results_dir)

        return {
            'config': config,
            'domain_terms': domain_terms,
            'filtered_topics': filtered_topics,
            'matching_repos': matching_repos
        }

    def _generate_report(self):
        """Generate a comprehensive report of all domains."""
        print("\n" + "="*70)
        print("COMBINED RESULTS REPORT")
        print("="*70)

        total_repos = 0
        for domain, results in self.results.items():
            if not results:
                continue

            filtered_topics = results.get('filtered_topics', [])
            matching_repos = results.get('matching_repos', [])
            total_repos += len(matching_repos)

            print(f"\n{domain.upper()}:")
            print(f"  - Domain terms: {len(results.get('domain_terms', []))}")
            print(f"  - Filtered topics: {len(filtered_topics)}")
            print(f"  - Matching repositories: {len(matching_repos)}")

            if filtered_topics:
                print(f"  - Top 5 topics:")
                for i, topic in enumerate(filtered_topics[:5]):
                    print(f"    {i+1}. {topic['topic']} (score: {topic.get('combined_score', 0):.3f})")

        print(f"\nTotal repositories found across all domains: {total_repos:,}")

# ===============================
# EXECUTION
# ===============================
if __name__ == "__main__":
    # Install required packages if not available
    try:
        import transformers
        import sentence_transformers
        print("Required packages are available.")
    except ImportError:
        print("Some packages may need to be installed:")
        print("pip install transformers sentence-transformers torch")

    # Configure which domains to process
    domains_to_process = ['math', 'physics', 'biology']  # Adjust as needed

    # Create and run pipeline
    pipeline = MultiDomainPipeline(domains=domains_to_process, data_dir=".")
    pipeline.run()

# ===============================
# MINIMAL INFOMANIAK UPLOADER
# ===============================
import requests
import os
from pathlib import Path


def upload_to_infomaniak():
    """Upload all result files to Infomaniak using v3 API."""

    # YOUR CREDENTIALS - Use the same as in your working code
    TOKEN = "Fi9Aaadhibox6l50TcideMovSUo8JtaKc30i_yP-XJFcf_db9HL_BX9KQ0HZN41H8u53Pu6tGRq3PGx5"
    DRIVE_ID = "705884"
    FOLDER_ID = "20521"  # Your upload folder ID

    # Your prefix
    FILE_PREFIX = "SCIBERT_filtering_"

    # Results directories
    RESULTS_DIR = "results"
    DOMAINS = ['math', 'physics', 'biology']

    # API endpoint - MATCHING YOUR WORKING CODE
    UPLOAD_URL = f"https://api.infomaniak.com/3/drive/{DRIVE_ID}/upload"

    print(f"\nUploading files from {RESULTS_DIR}/ to Infomaniak Drive...")

    for domain in DOMAINS:
        domain_dir = os.path.join(RESULTS_DIR, domain)

        if not os.path.exists(domain_dir):
            print(f"Directory not found: {domain_dir}")
            continue

        print(f"\n{domain.upper()} files:")

        for filename in os.listdir(domain_dir):
            filepath = os.path.join(domain_dir, filename)

            if os.path.isfile(filepath):
                try:
                    # Add your prefix
                    upload_filename = f"{FILE_PREFIX}{filename}"

                    # Get file size
                    file_size = os.path.getsize(filepath)
                    print(f"  ðŸ“¤ {upload_filename} ({file_size:,} bytes)...")

                    # Read file content
                    with open(filepath, 'rb') as f:
                        file_content = f.read()

                    # Prepare parameters - MATCHING YOUR WORKING CODE
                    params = {
                        "total_size": len(file_content),
                        "directory_id": FOLDER_ID,
                        "file_name": upload_filename
                    }

                    headers = {
                        "Authorization": f"Bearer {TOKEN}",
                        # "Content-Type": "application/octet-stream"  # NOT needed for multipart
                    }

                    # Make the request - passing params and data separately
                    response = requests.post(
                        UPLOAD_URL,
                        headers=headers,
                        params=params,  # parameters go here
                        data=file_content  # file data goes here
                    )

                    if response.status_code == 200:
                        result = response.json()
                        if result.get('result') == 'success':
                            print(f"    âœ“ Uploaded successfully")
                        else:
                            print(f"    âœ— Failed: {result.get('error', 'Unknown error')}")
                    else:
                        print(f"    âœ— HTTP {response.status_code}: {response.text[:100]}")

                except Exception as e:
                    print(f"    âœ— Error: {str(e)[:80]}")

    print("\nUpload complete!")


# ===============================
# EVEN SIMPLER VERSION
# ===============================
def simple_upload():
    """Simplest version using the exact same pattern as your working code."""

    TOKEN = "Fi9Aaadhibox6l50TcideMovSUo8JtaKc30i_yP-XJFcf_db9HL_BX9KQ0HZN41H8u53Pu6tGRq3PGx5"
    DRIVE_ID = "705884"
    FOLDER_ID = "20521"
    PREFIX = "SCIBERT_filtering_"

    UPLOAD_URL = f"https://api.infomaniak.com/3/drive/{DRIVE_ID}/upload"

    print("Uploading result files...")

    # Find all result files
    for root, dirs, files in os.walk("results"):
        for file in files:
            if file.endswith(('.jsonl', '.json', '.txt')):
                filepath = os.path.join(root, file)
                domain = os.path.basename(root)
                upload_filename = f"{PREFIX}{domain}_{file}"

                print(f"Uploading: {upload_filename}")

                try:
                    with open(filepath, 'rb') as f:
                        content = f.read()

                    response = requests.post(
                        UPLOAD_URL,
                        headers={"Authorization": f"Bearer {TOKEN}"},
                        params={
                            "total_size": len(content),
                            "directory_id": FOLDER_ID,
                            "file_name": upload_filename
                        },
                        data=content
                    )

                    if response.ok:
                        print(f"  âœ“ Success")
                    else:
                        print(f"  âœ— Failed: {response.status_code}")

                except Exception as e:
                    print(f"  âœ— Error: {e}")


# ===============================
# MAIN EXECUTION
# ===============================
if __name__ == "__main__":
    # Upload results
    if os.path.exists("results"):
        print("Found results directory, uploading...")
        simple_upload()  # Use the simpler version
    else:
        print("Results directory not found.")

# -*- coding: utf-8 -*-
"""MultiDomainSoftwareReposFinder.ipynb

Automatically generated pipeline for finding mathematical, physics, and biology software repositories.
"""
# ===============================
# BASELINE 2
# ===============================
# ===============================
# IMPORTS & SETUP
# ===============================
import os
import json
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
import math
import warnings
import tarfile
import zstandard as zstd
from pathlib import Path
from typing import List, Dict, Tuple, Set, Any, Optional
from collections import Counter
from tqdm.auto import tqdm
from multiprocessing import Pool, cpu_count
from sentence_transformers import SentenceTransformer
warnings.filterwarnings('ignore')

# ===============================
# CONFIGURATION
# ===============================
class DomainConfig:
    """Configuration for different scientific domains."""

    DOMAINS = {
        'math': {
            'name': 'Mathematics',
            'term_file': 'mathematics_keywords.txt',
            'context_prefix': 'mathematical concept: ',
            'similarity_threshold': 0.6,
            'min_topics': 1000,
            'max_topics': 5000
        },
        'physics': {
            'name': 'Physics',
            'term_file': 'physics_keywords.txt',
            'context_prefix': 'physics concept: ',
            'similarity_threshold': 0.6,
            'min_topics': 1000,
            'max_topics': 5000
        },
        'biology': {
            'name': 'Biology',
            'term_file': 'biology_keywords.txt',
            'context_prefix': 'biological concept: ',
            'similarity_threshold': 0.6,
            'min_topics': 1000,
            'max_topics': 5000
        }
    }

    @classmethod
    def get_domain_config(cls, domain: str) -> Dict:
        """Get configuration for a specific domain."""
        if domain not in cls.DOMAINS:
            raise ValueError(f"Domain must be one of: {list(cls.DOMAINS.keys())}")
        return cls.DOMAINS[domain]

# ===============================
# DOMAIN TERM LOADER
# ===============================
class DomainTermLoader:
    """Loads and processes domain-specific terms."""

    @staticmethod
    def load_terms(term_file: str) -> List[str]:
        """Load domain terms from file."""
        try:
            # Try multiple possible locations
            possible_paths = [
                Path(term_file),
                Path("downloaded_files") / term_file,
                Path("data") / term_file,
                Path(".") / term_file
            ]

            file_path = None
            for path in possible_paths:
                if path.exists():
                    file_path = path
                    break

            if file_path is None:
                print(f"Warning: Term file {term_file} not found in any expected location.")
                return []

            with open(file_path, "r", encoding='utf-8') as f:
                content = f.read()
                # Handle different separators
                if "/n" in content:
                    terms = [term.strip() for term in content.split("/n") if term.strip()]
                else:
                    terms = [term.strip() for term in content.split("\n") if term.strip()]

            print(f"Loaded {len(terms)} terms from {file_path}")
            if terms:
                print(f"First 5 terms: {terms[:5]}")
            return terms
        except Exception as e:
            print(f"Error loading terms from {term_file}: {e}")
            return []

# ===============================
# EMBEDDING MODEL
# ===============================
class EmbeddingModel:
    """Handles embedding model loading and encoding."""

    def __init__(self, model_name: str = 'malteos/scincl', device: str = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.tokenizer = None
        self.load_model(model_name)

    def load_model(self, model_name: str):
        """Load the embedding model."""
        print(f"Using device: {self.device}")

        try:
            if model_name == 'malteos/scincl':
                print("Loading SCINCL model...")
                from transformers import AutoTokenizer, AutoModel

                self.tokenizer = AutoTokenizer.from_pretrained('malteos/scincl')
                self.model = AutoModel.from_pretrained('malteos/scincl').to(self.device)
                self.model.eval()
                print("SCINCL loaded successfully!")
            else:
                # Fallback to sentence transformer
                print(f"Loading model: {model_name}")
                self.model = SentenceTransformer(model_name).to(self.device)
                print(f"{model_name} loaded successfully!")

        except Exception as e:
            print(f"Error loading model: {e}")
    def encode(self, texts: List[str], batch_size: int = 256,
               convert_to_tensor: bool = True, **kwargs) -> torch.Tensor:
        """Encode texts to embeddings."""
        if not texts:
            raise ValueError("Cannot encode empty list of texts")

        if hasattr(self.model, 'encode') and callable(self.model.encode):
            # For SentenceTransformer models
            result = self.model.encode(
                texts,
                convert_to_tensor=convert_to_tensor,
                show_progress_bar=True,
                batch_size=batch_size,
                normalize_embeddings=True
            )
            if convert_to_tensor and not isinstance(result, torch.Tensor):
                result = torch.tensor(result, device=self.device)
            return result.to(self.device) if convert_to_tensor else result
        else:
            # For SPECTER2 models
            self.model.eval()
            all_embeddings = []

            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512
                ).to(self.device)

                with torch.no_grad():
                    outputs = self.model(**inputs)
                    embeddings = outputs.last_hidden_state[:, 0, :]
                    embeddings = F.normalize(embeddings, p=2, dim=1)
                    all_embeddings.append(embeddings.cpu())

            result = torch.cat(all_embeddings)
            return result.to(self.device) if convert_to_tensor else result

# ===============================
# SIMILARITY-BASED FILTERING
# ===============================
class SimilarityFilter:
    """Direct similarity-based filtering using SCINCL embeddings."""

    def __init__(self, domain_config: Dict, embedding_model: EmbeddingModel):
        self.domain_config = domain_config
        self.encoder = embedding_model
        self.domain_embeddings = None
        self.domain_centroid = None

    def encode_domain_terms(self, domain_terms: List[str]) -> torch.Tensor:
        """Encode domain terms with context augmentation."""
        print(f"Encoding {len(domain_terms)} domain terms...")
        context_prefix = self.domain_config['context_prefix']
        domain_terms_with_context = [f"{context_prefix}{term}" for term in domain_terms]
        embeddings = self.encoder.encode(domain_terms_with_context, batch_size=128)
        self.domain_embeddings = embeddings
        self.domain_centroid = embeddings.mean(dim=0, keepdim=True)
        return embeddings

    def compute_similarities(self, topics: List[str]) -> Dict[str, Any]:
        """Compute various similarity metrics for topics."""
        print(f"Encoding {len(topics)} topics...")

        # Encode topics with context
        topics_with_context = [f"topic: {topic}" for topic in topics]
        topic_embeddings = self.encoder.encode(
            topics_with_context,
            batch_size=512,
            convert_to_tensor=True
        )

        # Normalize embeddings
        topic_emb_norm = F.normalize(topic_embeddings, p=2, dim=1)
        domain_emb_norm = F.normalize(self.domain_embeddings, p=2, dim=1)

        # Compute similarity matrix
        print("Computing similarities...")
        similarity_matrix = torch.mm(topic_emb_norm, domain_emb_norm.T)

        # Compute various similarity metrics
        max_similarities, closest_indices = similarity_matrix.max(dim=1)
        avg_similarities = similarity_matrix.mean(dim=1)

        # Top-k similarities
        k = min(5, len(self.domain_embeddings))
        top_k_similarities, _ = similarity_matrix.topk(k=k, dim=1)
        avg_top_k = top_k_similarities.mean(dim=1)

        # Centroid similarity
        centroid_similarities = F.cosine_similarity(
            topic_emb_norm,
            self.domain_centroid
        )

        # Combined score
        combined_scores = (
            0.35 * max_similarities +
            0.25 * avg_similarities +
            0.20 * avg_top_k +
            0.20 * centroid_similarities
        )

        return {
            'topics': topics,
            'topic_embeddings': topic_embeddings,
            'similarity_matrix': similarity_matrix,
            'max_similarities': max_similarities,
            'avg_similarities': avg_similarities,
            'top_k_similarities': top_k_similarities,
            'avg_top_k': avg_top_k,
            'centroid_similarities': centroid_similarities,
            'combined_scores': combined_scores,
            'closest_indices': closest_indices
        }

    def filter_topics(self, all_topics: List[str],
                     domain_terms: List[str]) -> List[Dict]:
        """Filter topics using direct similarity metrics."""
        print(f"\n{'='*70}")
        print(f"Processing {len(all_topics)} topics for {self.domain_config['name']}")
        print('='*70)

        # Encode domain terms
        self.encode_domain_terms(domain_terms)

        # Process in chunks to manage memory
        chunk_size = 50000
        all_results = []

        for i in range(0, len(all_topics), chunk_size):
            chunk_topics = all_topics[i:i+chunk_size]
            print(f"Processing chunk {i//chunk_size + 1}/{(len(all_topics) + chunk_size - 1)//chunk_size}")

            # Compute similarities for chunk
            sim_results = self.compute_similarities(chunk_topics)

            # Create result dictionaries
            for j, topic in enumerate(chunk_topics):
                all_results.append({
                    'topic': topic,
                    'max_similarity': sim_results['max_similarities'][j].item(),
                    'avg_similarity': sim_results['avg_similarities'][j].item(),
                    'avg_top5_similarity': sim_results['avg_top_k'][j].item(),
                    'centroid_similarity': sim_results['centroid_similarities'][j].item(),
                    'combined_score': sim_results['combined_scores'][j].item(),
                    'closest_domain_term': domain_terms[sim_results['closest_indices'][j].item()],
                    'domain': self.domain_config['name']
                })

            # Memory cleanup
            del sim_results
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        # Sort by combined score
        all_results.sort(key=lambda x: x['combined_score'], reverse=True)

        # Adaptive thresholding
        combined_scores = np.array([r['combined_score'] for r in all_results])
        threshold = np.percentile(combined_scores, 85)

        # Filter by threshold and config limits
        config = self.domain_config
        filtered_results = []

        for result in all_results:
            if (result['combined_score'] >= threshold or
                len(filtered_results) < config['min_topics']):
                filtered_results.append(result)
            if len(filtered_results) >= config['max_topics']:
                break

        print(f"\nFiltering results:")
        print(f"  Total topics processed: {len(all_results)}")
        print(f"  Score threshold (85th percentile): {threshold:.3f}")
        print(f"  Topics after filtering: {len(filtered_results)}")
        print(f"  Top 5 topics:")
        for i, r in enumerate(filtered_results[:5]):
            print(f"    {i+1}. {r['topic']} (score: {r['combined_score']:.3f})")

        return filtered_results

# ===============================
# REPOSITORY FINDER
# ===============================
class RepositoryFinder:
    """Finds repositories related to specific domains."""

    def __init__(self, domain: str, filtered_topics: List[Dict]):
        self.domain = domain
        self.filtered_topics = filtered_topics
        self.strict_topic_set = self.create_strict_topic_set()
        self.topic_idf_scores = None

    def create_strict_topic_set(self) -> Set[str]:
        """Create strict set of domain topics based on similarity metrics."""
        strict_topics = set()
        config = DomainConfig.get_domain_config(self.domain)
        threshold = config.get('similarity_threshold', 0.6)

        for topic_data in self.filtered_topics:
            if topic_data['combined_score'] >= threshold:
                strict_topics.add(topic_data['topic'])

        print(f"Created strict topic set with {len(strict_topics)} topics")
        print(f"Threshold: {threshold}")
        return strict_topics

    def compute_idf_scores(self, filename: str = 'repos.ndjson',
                          n_workers: int = None) -> Dict[str, float]:
        """Compute IDF scores for all topics."""
        n_workers = n_workers or cpu_count()
        n_chunks = n_workers * 4

        print("Computing IDF scores...")

        # Read file in chunks
        chunks, total_lines = self._read_file_chunks(filename, n_chunks)

        # Count topics in parallel
        with Pool(n_workers) as pool:
            topic_counters = list(tqdm(
                pool.imap_unordered(self._count_topics_in_chunk, chunks),
                total=len(chunks),
                desc="Counting topics"
            ))

        # Merge counters and compute IDF
        all_topics_counter = Counter()
        for counter in topic_counters:
            all_topics_counter.update(counter)

        idf_scores = {}
        for topic, doc_freq in all_topics_counter.items():
            idf_scores[topic] = math.log(total_lines / doc_freq)

        self.topic_idf_scores = idf_scores
        print(f"Computed IDF for {len(idf_scores):,} unique topics")
        return idf_scores

    def find_repositories(self, filename: str = 'repos.ndjson',
                         n_workers: int = None) -> List[Dict]:
        """Find repositories containing domain topics."""
        n_workers = n_workers or cpu_count()
        n_chunks = n_workers * 4

        if self.topic_idf_scores is None:
            self.compute_idf_scores(filename, n_workers)

        print(f"\nFinding {self.domain} repositories...")

        # Read file in chunks
        chunks, total_lines = self._read_file_chunks(filename, n_chunks)

        # Process in parallel
        with Pool(n_workers, initializer=self._init_worker,
                  initargs=(self.topic_idf_scores, self.strict_topic_set)) as pool:
            results = list(tqdm(
                pool.imap_unordered(self._process_chunk, chunks),
                total=len(chunks),
                desc=f"Finding {self.domain} repos"
            ))

        # Merge results
        all_matching = []
        domain_topics_counter = Counter()

        for matching, counter in results:
            all_matching.extend(matching)
            domain_topics_counter.update(counter)

        # Analyze results
        print(f"\n{'='*60}")
        print(f"{self.domain.upper()} REPOSITORY FINDING RESULTS")
        print('='*60)
        print(f"Total matching repositories: {len(all_matching):,}")
        print(f"Percentage of total: {len(all_matching)/total_lines*100:.2f}%")

        print(f"\nTop 20 detected {self.domain} topics:")
        for topic, count in domain_topics_counter.most_common(20):
            print(f"  {topic}: {count:,}")

        return all_matching

    def save_results(self, matching_repos: List[Dict],
                    output_dir: str = "results"):
        """Save results to files."""
        os.makedirs(output_dir, exist_ok=True)

        # Save matching repositories
        output_file = os.path.join(output_dir, f"{self.domain}_repos.jsonl")
        with open(output_file, 'w', encoding='utf-8') as f:
            for repo in matching_repos:
                f.write(json.dumps(repo) + '\n')

        # Save filtered topics
        topics_file = os.path.join(output_dir, f"{self.domain}_filtered_topics.json")
        with open(topics_file, 'w') as f:
            json.dump(self.filtered_topics, f, indent=2)

        # Save strict topics
        strict_topics_file = os.path.join(output_dir, f"{self.domain}_topics.txt")
        with open(strict_topics_file, 'w') as f:
            for topic in sorted(self.strict_topic_set):
                f.write(f"{topic}\n")

        print(f"\nSaved results to {output_dir}:")
        print(f"  - {output_file}")
        print(f"  - {topics_file}")
        print(f"  - {strict_topics_file}")

    # Helper methods for parallel processing
    @staticmethod
    def _read_file_chunks(filename: str, num_chunks: int) -> Tuple[List[List[str]], int]:
        """Read file and split into chunks."""
        print("Loading file into memory...")
        with open(filename, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()

        total_lines = len(all_lines)
        chunk_size = total_lines // num_chunks + 1

        chunks = []
        for i in range(0, total_lines, chunk_size):
            chunks.append(all_lines[i:i + chunk_size])

        return chunks, total_lines

    @staticmethod
    def _count_topics_in_chunk(lines: List[str]) -> Counter:
        """Count topics in a chunk."""
        counter = Counter()
        for line in lines:
            try:
                data = json.loads(line.strip())
                topics = data.get('topics', [])
                if topics:
                    counter.update(topics)
            except:
                pass
        return counter

    @staticmethod
    def _process_chunk(lines: List[str]) -> Tuple[List[Dict], Counter]:
        """Process chunk to find domain repositories."""
        matching = []
        topic_counter = Counter()

        global _worker_idf_scores, _worker_topic_set

        for line in lines:
            try:
                data = json.loads(line.strip())
                topics = data.get('topics', [])

                if not topics:
                    continue

                # Get only domain topics
                domain_topics = [t for t in topics if t in _worker_topic_set]

                if not domain_topics:
                    continue

                # Calculate scores
                domain_score = sum(_worker_idf_scores.get(t, 0) for t in domain_topics)
                total_score = sum(_worker_idf_scores.get(t, 0) for t in topics)

                if total_score > 0:
                    domain_ratio = domain_score / total_score

                    # Adaptive threshold
                    num_topics = len(topics)
                    if num_topics <= 3:
                        threshold = 0.4
                    elif num_topics <= 6:
                        threshold = 0.35
                    else:
                        threshold = 0.3

                    if domain_ratio >= threshold:
                        matching.append({
                            'swhid': data.get('swhid'),
                            'topics': topics,
                            'url': data.get('url', []),
                            'domain_topics': domain_topics,
                            'domain_ratio': round(domain_ratio, 3),
                            'num_topics': num_topics,
                            'domain': 'unknown'  # Placeholder
                        })
                        topic_counter.update(domain_topics)
            except:
                pass

        return matching, topic_counter

    @staticmethod
    def _init_worker(idf_dict: Dict, topic_set: Set):
        """Initialize worker process."""
        global _worker_idf_scores, _worker_topic_set
        _worker_idf_scores = idf_dict
        _worker_topic_set = topic_set

# ===============================
# BLOB COLLECTOR
# ===============================
class BlobCollector:
    """Collects blob files for matching repositories."""

    @staticmethod
    def build_blob_paths(matching_repos: List[Dict],
                        extraction_dir: str = "extracted_data") -> List[str]:
        """Build blob file paths from matching repositories."""
        blob_paths = []

        for repo in matching_repos:
            readme = repo.get('readme')
            if readme:
                # Extract hash from readme (format: swh:1:cnt:hash)
                hash_part = readme.split(':')[-1]

                if hash_part and len(hash_part) >= 4:
                    first_two = hash_part[:2]
                    next_two = hash_part[2:4]

                    blob_pattern = f"{extraction_dir}/blobs-by-swhid/{first_two}/{next_two}/{hash_part}"
                    blob_paths.append(blob_pattern)

        print(f"Built {len(blob_paths)} blob paths")
        return blob_paths

    @staticmethod
    def collect_files(blob_paths: List[str], output_dir: str = "collected_blobs"):
        """Collect files into directory."""
        os.makedirs(output_dir, exist_ok=True)
        collected_count = 0

        for blob_path in blob_paths:
            if os.path.exists(blob_path):
                # Copy file to output directory
                import shutil
                shutil.copy2(blob_path, output_dir)
                collected_count += 1

        print(f"Collected {collected_count} files to {output_dir}")
        return collected_count

# ===============================
# MAIN PIPELINE
# ===============================
class MultiDomainPipeline:
    """Main pipeline for processing multiple scientific domains."""

    def __init__(self, domains: List[str] = None, data_dir: str = "data"):
        if domains is None:
            domains = ['math', 'physics', 'biology']
        self.domains = domains
        self.data_dir = data_dir
        self.results = {}
        os.makedirs(data_dir, exist_ok=True)

    def run(self):
        """Run the complete pipeline for all domains."""
        print("="*70)
        print("MULTI-DOMAIN SOFTWARE REPOSITORY PIPELINE")
        print("="*70)

        # Step 1: Load all topics from GitHub data
        print("\n[STEP 1] Loading GitHub topics...")
        repos_file = os.path.join(self.data_dir, "repos.ndjson")
        if not os.path.exists(repos_file):
            print(f"Error: GitHub topics file not found at {repos_file}")
            print("Please ensure 'repos.ndjson' is in the data directory.")
            return

        all_topics, topic_counter = self._load_all_topics(repos_file)

        # Step 2: Initialize embedding model
        print("\n[STEP 2] Initializing embedding model...")
        embedding_model = EmbeddingModel('malteos/scincl')

        # Step 3: Process each domain
        for domain in self.domains:
            print(f"\n{'='*70}")
            print(f"PROCESSING DOMAIN: {domain.upper()}")
            print('='*70)

            try:
                domain_results = self._process_domain(
                    domain, all_topics, embedding_model
                )
                self.results[domain] = domain_results

                print(f"\nâœ… Completed {domain.upper()} processing!")
                print(f"   Found {len(domain_results['filtered_topics'])} domain topics")
                print(f"   Found {len(domain_results['matching_repos'])} repositories")

            except Exception as e:
                print(f"âŒ Error processing {domain}: {e}")
                import traceback
                traceback.print_exc()
                continue

        # Step 4: Generate combined report
        self._generate_report()

        # Step 5: Collect blob files for each domain (optional)
        print("\n[STEP 5] Collecting blob files...")
        extraction_dir = os.path.join(self.data_dir, "extracted_data")
        if os.path.exists(extraction_dir):
            for domain, results in self.results.items():
                if results and 'matching_repos' in results:
                    blob_paths = BlobCollector.build_blob_paths(
                        results['matching_repos'], extraction_dir
                    )
                    output_dir = os.path.join(self.data_dir, f"{domain}_blobs")
                    collected = BlobCollector.collect_files(blob_paths, output_dir)
                    print(f"  {domain}: collected {collected} blobs")
        else:
            print("Blob extraction directory not found. Skipping blob collection.")

        print("\n" + "="*70)
        print("PIPELINE COMPLETE!")
        print("="*70)

    def _load_all_topics(self, repos_file: str) -> Tuple[List[str], Counter]:
        """Load all topics from the GitHub dataset."""
        topic_counter = Counter()

        with open(repos_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="Loading topics"):
                try:
                    data = json.loads(line.strip())
                    topics = data.get('topics', [])
                    if topics:
                        topic_counter.update(topics)
                except:
                    pass

        all_topics = list(topic_counter.keys())
        print(f"Loaded {len(all_topics)} unique topics")
        print(f"Total topic occurrences: {sum(topic_counter.values()):,}")
        return all_topics, topic_counter

    def _process_domain(self, domain: str, all_topics: List[str],
                       embedding_model: EmbeddingModel) -> Dict:
        """Process a single domain."""
        config = DomainConfig.get_domain_config(domain)

        # Load domain terms
        term_loader = DomainTermLoader()
        domain_terms = term_loader.load_terms(config['term_file'])

        if not domain_terms:
            print(f"Warning: No terms loaded for {domain}")
            return {}

        # Apply similarity filtering
        similarity_filter = SimilarityFilter(config, embedding_model)
        filtered_topics = similarity_filter.filter_topics(all_topics, domain_terms)

        # Find repositories
        repo_finder = RepositoryFinder(domain, filtered_topics)
        repos_file = os.path.join(self.data_dir, "repos.ndjson")
        matching_repos = repo_finder.find_repositories(repos_file)

        # Save results
        results_dir = os.path.join(self.data_dir, "results", domain)
        repo_finder.save_results(matching_repos, results_dir)

        return {
            'config': config,
            'domain_terms': domain_terms,
            'filtered_topics': filtered_topics,
            'matching_repos': matching_repos
        }

    def _generate_report(self):
        """Generate a comprehensive report of all domains."""
        print("\n" + "="*70)
        print("COMBINED RESULTS REPORT")
        print("="*70)

        total_repos = 0
        for domain, results in self.results.items():
            if not results:
                continue

            filtered_topics = results.get('filtered_topics', [])
            matching_repos = results.get('matching_repos', [])
            total_repos += len(matching_repos)

            print(f"\n{domain.upper()}:")
            print(f"  - Domain terms: {len(results.get('domain_terms', []))}")
            print(f"  - Filtered topics: {len(filtered_topics)}")
            print(f"  - Matching repositories: {len(matching_repos)}")

            if filtered_topics:
                print(f"  - Top 5 topics:")
                for i, topic in enumerate(filtered_topics[:5]):
                    print(f"    {i+1}. {topic['topic']} (score: {topic.get('combined_score', 0):.3f})")

        print(f"\nTotal repositories found across all domains: {total_repos:,}")

# ===============================
# EXECUTION
# ===============================
if __name__ == "__main__":
    # Install required packages if not available
    try:
        import transformers
        import sentence_transformers
        print("Required packages are available.")
    except ImportError:
        print("Some packages may need to be installed:")
        print("pip install transformers sentence-transformers torch")

    # Configure which domains to process
    domains_to_process = ['math', 'physics', 'biology']  # Adjust as needed

    # Create and run pipeline
    pipeline = MultiDomainPipeline(domains=domains_to_process, data_dir=".")
    pipeline.run()

# ===============================
# MINIMAL INFOMANIAK UPLOADER
# ===============================
import requests
import os
from pathlib import Path


def upload_to_infomaniak():
    """Upload all result files to Infomaniak using v3 API."""

    # YOUR CREDENTIALS - Use the same as in your working code
    TOKEN = "Fi9Aaadhibox6l50TcideMovSUo8JtaKc30i_yP-XJFcf_db9HL_BX9KQ0HZN41H8u53Pu6tGRq3PGx5"
    DRIVE_ID = "705884"
    FOLDER_ID = "20521"  # Your upload folder ID

    # Your prefix
    FILE_PREFIX = "SCINL_filtering_"

    # Results directories
    RESULTS_DIR = "results"
    DOMAINS = ['math', 'physics', 'biology']

    # API endpoint - MATCHING YOUR WORKING CODE
    UPLOAD_URL = f"https://api.infomaniak.com/3/drive/{DRIVE_ID}/upload"

    print(f"\nUploading files from {RESULTS_DIR}/ to Infomaniak Drive...")

    for domain in DOMAINS:
        domain_dir = os.path.join(RESULTS_DIR, domain)

        if not os.path.exists(domain_dir):
            print(f"Directory not found: {domain_dir}")
            continue

        print(f"\n{domain.upper()} files:")

        for filename in os.listdir(domain_dir):
            filepath = os.path.join(domain_dir, filename)

            if os.path.isfile(filepath):
                try:
                    # Add your prefix
                    upload_filename = f"{FILE_PREFIX}{filename}"

                    # Get file size
                    file_size = os.path.getsize(filepath)
                    print(f"  ðŸ“¤ {upload_filename} ({file_size:,} bytes)...")

                    # Read file content
                    with open(filepath, 'rb') as f:
                        file_content = f.read()

                    # Prepare parameters - MATCHING YOUR WORKING CODE
                    params = {
                        "total_size": len(file_content),
                        "directory_id": FOLDER_ID,
                        "file_name": upload_filename
                    }

                    headers = {
                        "Authorization": f"Bearer {TOKEN}",
                        # "Content-Type": "application/octet-stream"  # NOT needed for multipart
                    }

                    # Make the request - passing params and data separately
                    response = requests.post(
                        UPLOAD_URL,
                        headers=headers,
                        params=params,  # parameters go here
                        data=file_content  # file data goes here
                    )

                    if response.status_code == 200:
                        result = response.json()
                        if result.get('result') == 'success':
                            print(f"    âœ“ Uploaded successfully")
                        else:
                            print(f"    âœ— Failed: {result.get('error', 'Unknown error')}")
                    else:
                        print(f"    âœ— HTTP {response.status_code}: {response.text[:100]}")

                except Exception as e:
                    print(f"    âœ— Error: {str(e)[:80]}")

    print("\nUpload complete!")


# ===============================
# EVEN SIMPLER VERSION
# ===============================
def simple_upload():
    """Simplest version using the exact same pattern as your working code."""

    TOKEN = "Fi9Aaadhibox6l50TcideMovSUo8JtaKc30i_yP-XJFcf_db9HL_BX9KQ0HZN41H8u53Pu6tGRq3PGx5"
    DRIVE_ID = "705884"
    FOLDER_ID = "20521"
    PREFIX = "SCINL_filtering_"

    UPLOAD_URL = f"https://api.infomaniak.com/3/drive/{DRIVE_ID}/upload"

    print("Uploading result files...")

    # Find all result files
    for root, dirs, files in os.walk("results"):
        for file in files:
            if file.endswith(('.jsonl', '.json', '.txt')):
                filepath = os.path.join(root, file)
                domain = os.path.basename(root)
                upload_filename = f"{PREFIX}{domain}_{file}"

                print(f"Uploading: {upload_filename}")

                try:
                    with open(filepath, 'rb') as f:
                        content = f.read()

                    response = requests.post(
                        UPLOAD_URL,
                        headers={"Authorization": f"Bearer {TOKEN}"},
                        params={
                            "total_size": len(content),
                            "directory_id": FOLDER_ID,
                            "file_name": upload_filename
                        },
                        data=content
                    )

                    if response.ok:
                        print(f"  âœ“ Success")
                    else:
                        print(f"  âœ— Failed: {response.status_code}")

                except Exception as e:
                    print(f"  âœ— Error: {e}")


# ===============================
# MAIN EXECUTION
# ===============================
if __name__ == "__main__":
    # Upload results
    if os.path.exists("results"):
        print("Found results directory, uploading...")
        simple_upload()  # Use the simpler version
    else:
        print("Results directory not found.")