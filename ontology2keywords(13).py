# -*- coding: utf-8 -*-
"""Ontology2Keywords.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CwJuuF2xSpz9w8xDr9tBF480aHjGOk6A
"""

!pip install keybert git+https://github.com/bio-ontology-research-group/mowl rdflib

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"

# ========== IMPORTS ==========
import pandas as pd
import numpy as np
import torch
import json
import re
import h5py
import pickle
import requests
from collections import Counter
from typing import Dict, List, Tuple, Any, Optional

# KeyBERT and embeddings
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Ontology processing
import rdflib
from rdflib import Graph, URIRef, Literal, Namespace
from rdflib.namespace import RDF, RDFS, OWL

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# mOWL for ontology embeddings
import mowl
# Initialize mOWL JVM - moved before mowl imports that require it
mowl.init_jvm("2g")
from mowl.datasets import PathDataset
from mowl.projection import OWL2VecStarProjector
from mowl.models import ELEmbeddings
from mowl.owlapi import OWLAPIAdapter
from org.semanticweb.owlapi.model.parameters import Imports
from org.semanticweb.owlapi.model import IRI
from org.semanticweb.owlapi.vocab import XSDVocabulary


# ========== UNIFORM ONTOLOGY PROCESSOR ==========
class UniformOntologyProcessor:
    """Uniform processor for all scientific ontologies with OWL conversion and ELEmbeddings"""

    def __init__(self, domain: str, specter_model: str = 'allenai/specter'):
        """Initialize with domain-specific settings"""
        self.domain = domain
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.specter_model = SentenceTransformer(specter_model, device=self.device)
        self.keyword_model = KeyBERT(model=self.specter_model)

        # Domain-specific IRI bases
        self.iri_bases = {
            'mathematics': 'http://purl.org/msc/msc2020/',
            'physics': 'http://purl.org/physh/ontology/',
            'biology': 'http://purl.org/edam/ontology/'
        }

        # Domain-specific class hierarchies
        self.class_hierarchies = {
            'mathematics': ['Major_Area', 'Sub_Area', 'Specific_Topic'],
            'physics': ['Major_Field', 'Sub_Field', 'Specific_Concept'],
            'biology': ['Major_Topic', 'Sub_Topic', 'Specific_Operation']
        }

    def create_uniform_ontology(self, df: pd.DataFrame, output_path: str) -> Tuple[Any, pd.DataFrame]:
        """Create enhanced OWL ontology with hierarchical features (uniform for all domains)"""
        # Extract hierarchical features based on domain
        df_features = self.extract_hierarchical_features(df, self.domain)

        # Initialize OWL API
        adapter = OWLAPIAdapter()
        manager = adapter.owl_manager
        data_factory = manager.getOWLDataFactory()

        # Create ontology
        ontology_iri_base = self.iri_bases[self.domain]
        ontology_iri = IRI.create(ontology_iri_base)
        ontology = manager.createOntology(ontology_iri)

        # Define classes
        domain_classes = {}
        for class_name in self.class_hierarchies[self.domain]:
            domain_classes[class_name] = data_factory.getOWLClass(
                IRI.create(ontology_iri_base + class_name)
            )

        # Define properties
        has_parent = data_factory.getOWLObjectProperty(
            IRI.create(ontology_iri_base + "hasParent")
        )
        code_value = data_factory.getOWLDataProperty(
            IRI.create(ontology_iri_base + "codeValue")
        )
        concept_label = data_factory.getOWLDataProperty(
            IRI.create(ontology_iri_base + "conceptLabel")
        )
        concept_description = data_factory.getOWLDataProperty(
            IRI.create(ontology_iri_base + "conceptDescription")
        )
        hierarchy_level = data_factory.getOWLDataProperty(
            IRI.create(ontology_iri_base + "hierarchyLevel")
        )

        # Datatypes
        xsd_string = data_factory.getOWLDatatype(XSDVocabulary.STRING.getIRI())
        xsd_integer = data_factory.getOWLDatatype(XSDVocabulary.INTEGER.getIRI())

        # Create individuals
        concept_individuals = {}
        for _, row in df_features.iterrows():
            concept_id = str(row['code'])
            # Create safe IRI (replace special characters)
            safe_id = re.sub(r'[^ɐ-ʯᴀ-ᴥᴬ-ᵜᵢ-ᵥᵫ-ᵷᵹ-ᶿⁱⁿₐ-ₜⱥ-ⱦⱼ-ⱽꜢ-ꜯꝧ-ꝨꞋ-ꞌꞮ-Ʇꟷ-ꟸＡ-Ｚａ-ｚ\w\-]+', '_', concept_id)
            concept_iri = IRI.create(ontology_iri_base + "concept_" + safe_id)
            concept_individual = data_factory.getOWLNamedIndividual(concept_iri)
            concept_individuals[concept_id] = concept_individual

            # Determine class based on hierarchy depth
            hierarchy_depth = row['hierarchy_depth']
            if hierarchy_depth == 1:
                class_type = self.class_hierarchies[self.domain][0]
            elif hierarchy_depth == 2:
                class_type = self.class_hierarchies[self.domain][1]
            else:
                class_type = self.class_hierarchies[self.domain][2]

            # Add type assertion
            class_axiom = data_factory.getOWLClassAssertionAxiom(
                domain_classes[class_type],
                concept_individual
            )
            manager.addAxiom(ontology, class_axiom)

            # Add data properties
            properties = [
                (code_value, concept_id, xsd_string),
                (concept_label, str(row['text']), xsd_string),
                (concept_description, str(row.get('description', '')), xsd_string),
                (hierarchy_level, str(row['hierarchy_depth']), xsd_integer)
            ]

            for prop, value, datatype in properties:
                if value and value != 'nan':
                    literal = data_factory.getOWLLiteral(value, datatype)
                    axiom = data_factory.getOWLDataPropertyAssertionAxiom(
                        prop, concept_individual, literal
                    )
                    manager.addAxiom(ontology, axiom)

        # Add parent-child relationships
        relationships = self.build_parent_child_relationships(df_features, self.domain)
        for _, rel in relationships.iterrows():
            if rel['parent'] in concept_individuals and rel['child'] in concept_individuals:
                parent_axiom = data_factory.getOWLObjectPropertyAssertionAxiom(
                    has_parent,
                    concept_individuals[rel['child']],
                    concept_individuals[rel['parent']]
                )
                manager.addAxiom(ontology, parent_axiom)

        # Save ontology
        import os
        absolute_output_path = os.path.abspath(output_path)
        output_iri = IRI.create("file://" + absolute_output_path)
        manager.saveOntology(ontology, output_iri)

        print(f"Created uniform ontology for {self.domain} with {len(df_features)} concepts")
        return ontology, df_features

    def extract_hierarchical_features(self, df: pd.DataFrame, domain: str) -> pd.DataFrame:
        """Extract hierarchical features based on domain-specific patterns"""
        df_features = df.copy()

        if domain == 'mathematics':
            # MSC-specific patterns
            df_features['major_area'] = df_features['code'].str.extract(r'^(\d{2})')
            df_features['subarea_letter'] = df_features['code'].str.extract(r'(\D+)')[0].fillna('')
            df_features['specificity_level'] = df_features['code'].apply(
                lambda x: len([c for c in x if c.isdigit()])
            )
            df_features['is_general'] = df_features['code'].str.contains('XX').astype(int)

            def get_hierarchy_depth(code):
                if '-' in code and 'XX' in code:
                    return 1  # Major area
                elif 'xx' in code.lower():
                    return 2  # Sub-area
                else:
                    return 3  # Specific topic

        elif domain == 'physics':
            # PhySH patterns - simpler hierarchy
            df_features['major_area'] = df_features['code'].apply(lambda x: x.split('.')[0] if '.' in x else '')
            df_features['specificity_level'] = df_features['code'].apply(
                lambda x: len(x.split('.')) if '.' in x else 1
            )
            df_features['is_general'] = 0  # Default

            def get_hierarchy_depth(code):
                if '.' in code:
                    parts = code.split('.')
                    return len(parts)  # 1, 2, or 3
                else:
                    return 1  # Top-level

        elif domain == 'biology':
            # EDAM patterns
            df_features['major_area'] = df_features['code'].str[:3]  # First 3 chars
            df_features['specificity_level'] = df_features['code'].str.len()
            df_features['is_general'] = 0  # Default

            def get_hierarchy_depth(code):
                if len(code) <= 4:
                    return 1  # Top-level
                elif len(code) <= 6:
                    return 2  # Mid-level
                else:
                    return 3  # Specific

        df_features['hierarchy_depth'] = df_features['code'].apply(get_hierarchy_depth)
        df_features['code_length'] = df_features['code'].str.len()
        df_features['has_dot'] = df_features['code'].str.contains(r'\\.').astype(int)  # Fixed escape sequence
        df_features['has_hyphen'] = df_features['code'].str.contains('-').astype(int)

        return df_features

    def build_parent_child_relationships(self, df: pd.DataFrame, domain: str) -> pd.DataFrame:
        """Build parent-child relationships based on domain patterns"""
        relationships = []

        for _, row in df.iterrows():
            code = row['code']
            hierarchy_depth = row['hierarchy_depth']

            if domain == 'mathematics':
                # MSC hierarchy rules
                if '-' in code and 'XX' in code:
                    continue  # Top-level
                elif 'xx' in code.lower():
                    parent = code[:2] + '-XX'
                    relationships.append((parent, code, 'has_subarea'))
                else:
                    if len(code) == 5:
                        parent = code[:3] + 'xx'
                        relationships.append((parent, code, 'has_specific_topic'))

            elif domain == 'physics':
                # PhySH hierarchy based on dots
                if '.' in code:
                    parts = code.split('.')
                    if len(parts) > 1:
                        parent = '.'.join(parts[:-1])
                        relationships.append((parent, code, 'has_subfield'))

            elif domain == 'biology':
                # EDAM hierarchy based on code prefixes
                if len(code) > 4:
                    parent = code[:4]
                    relationships.append((parent, code, 'has_subtopic'))

        return pd.DataFrame(relationships, columns=['parent', 'child', 'relationship_type'])

    def generate_embeddings(self, ontology_path: str, output_prefix: str):
        """Generate ELEmbeddings for ontology (uniform for all domains)"""
        dataset = PathDataset(ontology_path)

        # Project ontology to graph
        projector = OWL2VecStarProjector(bidirectional_taxonomy=True)
        edges = projector.project(dataset.ontology)

        # Train ELEmbeddings
        model = ELEmbeddings(
            dataset,
            embed_dim=100,
            margin=0.1,
            device=self.device,
            epochs=30,
            batch_size=16384,
            learning_rate=0.001,
        )
        model.train()

        # Save embeddings uniformly
        self.save_embeddings_uniform(model, output_prefix)

        return model

    def save_embeddings_uniform(self, model: ELEmbeddings, filename_prefix: str):
        """Save embeddings uniformly using HDF5 and pickle"""
        hdf5_file = f"{filename_prefix}_embeddings.h5"
        pkl_file = f"{filename_prefix}_metadata.pkl"

        if hasattr(model, 'individual_index_dict') and hasattr(model, 'individual_embeddings'):
            individuals = list(model.individual_index_dict.keys())
            indices = list(model.individual_index_dict.values())

            sorted_indices = np.argsort(indices)
            individuals = [individuals[i] for i in sorted_indices]

            total_individuals = len(individuals)

            # Save embeddings to HDF5
            with h5py.File(hdf5_file, 'w') as hf:
                embeddings_dset = hf.create_dataset(
                    "embeddings",
                    shape=(total_individuals, 100),
                    dtype=np.float32,
                    chunks=(min(1000, total_individuals), 100),
                    compression="gzip"
                )

                max_uri_length = max(len(uri) for uri in individuals)
                uris_dset = hf.create_dataset(
                    "uris",
                    shape=(total_individuals,),
                    dtype=f'S{max_uri_length}',
                    chunks=(min(1000, total_individuals),),
                    compression="gzip"
                )

                for i, uri in enumerate(individuals):
                    if uri in model.individual_embeddings:
                        embedding = model.individual_embeddings[uri]
                        if isinstance(embedding, torch.Tensor):
                            embedding = embedding.detach().cpu().numpy()
                        embeddings_dset[i] = embedding
                        uris_dset[i] = uri

            # Save metadata
            metadata = {
                'individual_index_dict': dict(model.individual_index_dict),
                'embedding_dim': 100,
                'total_individuals': total_individuals,
                'hdf5_file': hdf5_file,
                'domain': self.domain,
                'timestamp': pd.Timestamp.now().isoformat()
            }

            with open(pkl_file, 'wb') as f:
                pickle.dump(metadata, f, protocol=4)

            print(f"Saved embeddings: {hdf5_file} ({total_individuals} individuals)")

    def load_embeddings(self, hdf5_file: str) -> Dict:
        """Load embeddings from HDF5 file"""
        embeddings_dict = {}

        with h5py.File(hdf5_file, 'r') as hf:
            embeddings_data = hf['embeddings'][:]
            uris_data = hf['uris'][:]

            for i in range(len(uris_data)):
                uri = uris_data[i].decode('utf-8')
                embeddings_dict[uri] = embeddings_data[i]

        return embeddings_dict

    def generate_keywords(self, df: pd.DataFrame, embeddings_path: str) -> Dict:
        """Generate keywords using embeddings and SPECTER"""
        # Load embeddings
        embeddings = self.load_embeddings(embeddings_path)

        # Prepare text data
        df['combined_text'] = (
            f"{self.domain.capitalize()} Concept: " + df['code'] + ". " +
            "Title: " + df['text'] + ". " +
            "Description: " + df.get('description', '').fillna('') + ". " +
            "Hierarchy Level: " + df['hierarchy_depth'].astype(str)
        )

        all_keywords = {}

        for i, row in df.iterrows():
            text = row['combined_text']

            # Extract keywords using KeyBERT with SPECTER
            keywords = self.keyword_model.extract_keywords(
                text,
                keyphrase_ngram_range=(1, 3),
                stop_words='english',
                top_n=8,
                diversity=0.7
            )

            all_keywords[row['code']] = {
                'primary_keywords': [kw[0] for kw in keywords],
                'text': row['text'],
                'hierarchy_depth': row['hierarchy_depth']
            }

        return all_keywords

    def clean_keywords(self, keywords_dict: Dict, domain: str) -> List[str]:
        """Clean and deduplicate keywords with domain-specific cleaning"""
        all_keywords = []

        for code, data in keywords_dict.items():
            all_keywords.extend(data['primary_keywords'])

        # Domain-specific cleaning patterns
        domain_patterns = {
            'mathematics': r'\b\d{2}[a-zA-Z](\d{2}|[a-zA-Z]{2})\b',
            'physics': r'\b(physics|phys\\.|physh)\b',
            'biology': r'\b(biology|bio\\.|edam)\b'
        }

        # Generic terms to remove
        generic_terms = {
            'analysis', 'method', 'data', 'process', 'system',
            'study', 'research', 'science', 'field', 'topic'
        }

        cleaned_keywords = []
        for kw in all_keywords:
            # Remove domain-specific patterns
            if domain in domain_patterns:
                kw = re.sub(domain_patterns[domain], '', kw, flags=re.IGNORECASE)

            kw = ' '.join(kw.split())  # Normalize whitespace

            # Filter based on length and generic terms
            if (kw and len(kw) > 2 and
                kw.lower() not in generic_terms and
                not kw.isnumeric()):
                cleaned_keywords.append(kw)

        # Semantic deduplication with SPECTER
        unique_keywords = list(set(cleaned_keywords))

        if len(unique_keywords) > 1:
            embeddings = self.specter_model.encode(
                unique_keywords,
                convert_to_numpy=True,
                normalize_embeddings=True
            )

            similarity_matrix = cosine_similarity(embeddings)
            threshold = 0.85

            filtered_keywords = []
            seen = set()

            for i, kw in enumerate(unique_keywords):
                if i not in seen:
                    filtered_keywords.append(kw)
                    # Mark similar items
                    for j in range(i + 1, len(unique_keywords)):
                        if similarity_matrix[i, j] >= threshold:
                            seen.add(j)
        else:
            filtered_keywords = unique_keywords

        return filtered_keywords

# ========== DOMAIN-SPECIFIC PARSERS ==========
class MathematicalParser:
    """Parser for Mathematical Subject Classification (MSC)"""

    def __init__(self):
        self.ontology_url = "https://msc2020.org/MSC_2020.csv"

    def load_and_parse(self) -> pd.DataFrame:
        """Load MSC ontology from CSV and parse"""
        !wget {self.ontology_url} -O MSC_2020.csv
        df = pd.read_csv('MSC_2020.csv', encoding='latin1', sep='\t')

        # Clean and structure the data
        df_clean = pd.DataFrame({
            'code': df['code'].astype(str).str.strip(),          # ← FIXED
            'text': df['text'].astype(str).str.strip(),          # ← FIXED
            'description': df['description'].fillna('').astype(str)  # ← FIXED
        })

        return df_clean

class PhysicalParser:
    """Parser for Physics Subject Headings (PhySH)"""

    def __init__(self):
        self.base_url = "https://physh.org"

    def load_and_parse(self) -> pd.DataFrame:
        """Fetch PhySH concepts and parse into DataFrame"""
        headers = {"Accept": "application/json"}
        response = requests.get(f"{self.base_url}/concepts", headers=headers, timeout=30)

        if response.status_code != 200:
            raise Exception(f"Failed to fetch PhySH concepts: {response.status_code}")

        concepts_data = response.json()

        # Parse concepts into DataFrame
        data = []
        for concept in concepts_data:
            if isinstance(concept, dict):
                concept_id = concept.get('id', '')
                label = concept.get('label', '')

                # Clean the concept ID
                if concept_id and label:
                    # Create a code from the ID
                    code = concept_id.replace('/', '.').replace('-', '.')

                    data.append({
                        'code': code[:20],  # Truncate for consistency
                        'text': label.strip(),
                        'description': concept.get('description', '')
                    })

        return pd.DataFrame(data)

class BiologicalParser:
    """Parser for EDAM biology ontology"""

    def __init__(self):
        self.edam_url = "https://data.bioontology.org/ontologies/EDAM/submissions/44/download?apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb"

    def load_and_parse(self) -> pd.DataFrame:
        """Download and parse EDAM ontology"""
        # Download the ontology
        response = requests.get(self.edam_url, timeout=30)
        response.raise_for_status()

        with open('edam_ontology.owl', 'wb') as f:
            f.write(response.content)

        # Parse the OWL file
        g = Graph()
        g.parse('edam_ontology.owl', format='xml')

        # Define namespaces
        OBO = Namespace("http://purl.obolibrary.org/obo/")

        # Extract concepts
        data = []
        for concept in g.subjects(RDF.type, OWL.Class):
            if isinstance(concept, URIRef) and 'edamontology' in str(concept):
                concept_id = str(concept).split('/')[-1] if '/' in str(concept) else str(concept)

                # Get label
                label = ''
                for lbl in g.objects(concept, RDFS.label):
                    label = str(lbl)

                # Get definition
                definition = ''
                for defn in g.objects(concept, OBO.IAO_0000115):
                    definition = str(defn)

                if label:
                    data.append({
                        'code': concept_id,
                        'text': label,
                        'description': definition
                    })

        return pd.DataFrame(data)

# ========== SIMPLIFIED PROCESSING PIPELINE ==========
def process_domain_uniformly_simple(domain: str, parser, output_prefix: str):
    """Simplified uniform processing pipeline for any domain"""
    print(f"\n{'='*60}")
    print(f"PROCESSING {domain.upper()} ONTOLOGY")
    print(f"{'='*60}")

    results = {}

    try:
        # Step 1: Parse domain-specific data
        print(f"1. Parsing {domain} data...")
        df = parser.load_and_parse()
        print(f"   Parsed {len(df)} concepts")

        # Step 2: Create uniform processor
        print(f"2. Creating uniform ontology structure...")
        processor = UniformOntologyProcessor(domain)

        # Step 3: Create enhanced OWL ontology
        ontology_file = f"{output_prefix}_enhanced.owl"
        print(f"3. Creating enhanced OWL ontology...")
        ontology, df_features = processor.create_uniform_ontology(df, ontology_file)
        print(f"   Created enhanced ontology: {ontology_file}")

        # Step 4: Generate embeddings
        print(f"4. Generating ELEmbeddings...")
        try:
            model = processor.generate_embeddings(ontology_file, output_prefix)
            print(f"   Generated embeddings: {output_prefix}_embeddings.h5")

            # Step 5: Generate keywords
            print(f"5. Extracting keywords...")
            embeddings_file = f"{output_prefix}_embeddings.h5"
            keywords_dict = processor.generate_keywords(df_features, embeddings_file)

            # Clean keywords
            clean_keywords = processor.clean_keywords(keywords_dict, domain)
            print(f"   Extracted {len(clean_keywords)} clean keywords")

            # Save keywords
            keywords_file = f"{domain}_keywords.txt"
            with open(keywords_file, 'w', encoding='utf-8') as f:
                for kw in clean_keywords:
                    f.write(f"{kw}\n")

            results = {
                'success': True,
                'dataframe': df_features,
                'keywords': clean_keywords,
                'count': len(clean_keywords),
                'artifacts': {
                    'ontology': ontology_file,
                    'embeddings': embeddings_file,
                    'metadata': f"{output_prefix}_metadata.pkl",
                    'keywords': keywords_file
                }
            }

        except Exception as e:
            print(f"   Warning: Embedding generation failed, using text-only keywords: {e}")
            # Fallback to text-only keyword extraction
            print(f"   Using text-only keyword extraction...")

            # Prepare text data
            df_features['combined_text'] = (
                f"{domain.capitalize()} Concept: " + df_features['code'] + ". " +
                "Title: " + df_features['text'] + ". " +
                "Description: " + df_features.get('description', '').fillna('')
            )

            # Extract keywords directly
            all_keywords = []
            for text in df_features['combined_text']:
                keywords = processor.keyword_model.extract_keywords(
                    text,
                    keyphrase_ngram_range=(1, 3),
                    stop_words='english',
                    top_n=5,
                    diversity=0.7
                )
                all_keywords.extend([kw[0] for kw in keywords])

            # Clean and deduplicate
            clean_keywords = processor.clean_keywords(
                {i: {'primary_keywords': [kw]} for i, kw in enumerate(all_keywords)},
                domain
            )

            # Save keywords
            keywords_file = f"{domain}_keywords.txt"
            with open(keywords_file, 'w', encoding='utf-8') as f:
                for kw in clean_keywords:
                    f.write(f"{kw}\n")

            results = {
                'success': False,
                'dataframe': df_features,
                'keywords': clean_keywords,
                'count': len(clean_keywords),
                'artifacts': {
                    'ontology': ontology_file,
                    'keywords': keywords_file,
                    'note': 'Embeddings failed, text-only keywords extracted'
                }
            }

    except Exception as e:
        print(f"   Error processing {domain}: {e}")
        results = {
            'success': False,
            'error': str(e),
            'keywords': [],
            'count': 0
        }

    return results

# ========== MAIN EXECUTION PIPELINE ==========
def main_pipeline_uniform():
    """Main pipeline with uniform processing for all domains"""
    print("="*60)
    print("UNIFIED SCIENTIFIC ONTOLOGY PROCESSING PIPELINE")
    print("="*60)

    results = {}
    artifacts = {}

    # Process each domain uniformly
    domains = [
        ('mathematics', MathematicalParser(), 'msc'),
        ('physics', PhysicalParser(), 'physh'),
        ('biology', BiologicalParser(), 'edam')
    ]

    for domain, parser, prefix in domains:
        domain_result = process_domain_uniformly_simple(domain, parser, prefix)
        results[domain] = domain_result

        if domain_result.get('artifacts'):
            artifacts[domain] = domain_result['artifacts']

    # Combine all keywords
    print(f"\n{'='*60}")
    print("COMBINING ALL DOMAINS")
    print(f"{'='*60}")

    all_keywords = []
    for domain in results:
        if results[domain].get('keywords'):
            all_keywords.extend(results[domain]['keywords'])

    unique_keywords = list(set(all_keywords))
    unique_keywords.sort()

    # Save combined keywords
    combined_file = 'all_scientific_keywords_uniform.txt'
    with open(combined_file, 'w', encoding='utf-8') as f:
        for kw in unique_keywords:
            f.write(f"{kw}\n")

    # Generate summary
    summary = {
        'mathematics': {
            'success': results.get('mathematics', {}).get('success', False),
            'count': results.get('mathematics', {}).get('count', 0),
            'artifacts': artifacts.get('mathematics', {}),
            'sample': results.get('mathematics', {}).get('keywords', [])[:20]
        },
        'physics': {
            'success': results.get('physics', {}).get('success', False),
            'count': results.get('physics', {}).get('count', 0),
            'artifacts': artifacts.get('physics', {}),
            'sample': results.get('physics', {}).get('keywords', [])[:20]
        },
        'biology': {
            'success': results.get('biology', {}).get('success', False),
            'count': results.get('biology', {}).get('count', 0),
            'artifacts': artifacts.get('biology', {}),
            'sample': results.get('biology', {}).get('keywords', [])[:20]
        },
        'combined': {
            'total': len(unique_keywords),
            'file': combined_file
        }
    }

    # Save summary
    summary_file = 'scientific_keywords_uniform_summary.json'
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    # Visualization
    print(f"\n{'='*60}")
    print("GENERATING VISUALIZATIONS")
    print(f"{'='*60}")

    try:
        # Word cloud for all keywords
        text = " ".join(unique_keywords)
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title('Scientific Keywords Word Cloud')
        plt.savefig('scientific_keywords_wordcloud.png', dpi=300, bbox_inches='tight')
        plt.show()

        # Distribution by domain
        domain_counts = {domain: results[domain].get('count', 0) for domain in results}

        plt.figure(figsize=(8, 6))
        plt.bar(domain_counts.keys(), domain_counts.values(), color=['blue', 'green', 'red'])
        plt.title('Number of Keywords by Scientific Domain')
        plt.ylabel('Count')
        plt.tight_layout()
        plt.savefig('keywords_by_domain.png', dpi=300, bbox_inches='tight')
        plt.show()

        print("   Visualizations saved as PNG files")

    except Exception as e:
        print(f"   Warning: Visualization generation failed: {e}")

    # Final summary
    print(f"\n{'='*60}")
    print("PIPELINE COMPLETE")
    print(f"{'='*60}")

    for domain in results:
        success = "\u2713" if results[domain].get('success') else "\u2717"
        count = results[domain].get('count', 0)
        print(f"{success} {domain.capitalize()}: {count} keywords")

    print(f"\nTotal combined: {len(unique_keywords)} unique keywords")

    print(f"\nGenerated files:")
    for domain in artifacts:
        print(f"\n{domain.capitalize()}:")
        for key, value in artifacts[domain].items():
            print(f"  {key}: {value}")

    print(f"\nCombined files:")
    print(f"  {combined_file}")
    print(f"  {summary_file}")
    print(f"  scientific_keywords_wordcloud.png")
    print(f"  keywords_by_domain.png")

    return results, artifacts

# ========== KDRIVE UPLOAD ==========
def upload_to_kdrive(file_path: str, drive_id: int = 705884, directory_id: int = 20476):
    """Upload file to KDrive"""
    try:
        TOKEN = "Fi9Aaadhibox6l50TcideMovSUo8JtaKc30i_yP-XJFcf_db9HL_BX9KQ0HZN41H8u53Pu6tGRq3PGx5"
        headers = {"Authorization": f"Bearer {TOKEN}"}

        with open(file_path, 'rb') as f:
            file_content = f.read()

        upload_url = f"https://api.infomaniak.com/3/drive/{drive_id}/upload"
        params = {
            "total_size": len(file_content),
            "directory_id": directory_id,
            "file_name": os.path.basename(file_path)
        }

        response = requests.post(upload_url, headers=headers, params=params, data=file_content)

        if response.status_code == 200:
            print(f"\u2713 Uploaded {file_path} to KDrive")
            return True
        else:
            print(f"\u2717 Failed to upload {file_path}: {response.status_code}")
            return False

    except Exception as e:
        print(f"\u2717 Upload error for {file_path}: {e}")
        return False

# ========== EXECUTE PIPELINE ==========
if __name__ == "__main__":
    # Run the uniform pipeline
    results, artifacts = main_pipeline_uniform()

    # Upload key files to KDrive
    print(f"\n{'='*60}")
    print("UPLOADING TO KDRIVE")
    print(f"{'='*60}")

    files_to_upload = []

    # Add domain-specific files
    for domain in artifacts:
        for key, file_path in artifacts[domain].items():
            if isinstance(file_path, str) and os.path.exists(file_path):
                files_to_upload.append(file_path)

    # Add combined files
    combined_files = [
        'all_scientific_keywords_uniform.txt',
        'scientific_keywords_uniform_summary.json',
        'scientific_keywords_wordcloud.png',
        'keywords_by_domain.png'
    ]

    for file in combined_files:
        if os.path.exists(file):
            files_to_upload.append(file)

    # Upload files
    uploaded_count = 0
    for file in files_to_upload:
        if upload_to_kdrive(file):
            uploaded_count += 1

    print(f"\n\u2705 Uploaded {uploaded_count}/{len(files_to_upload)} files to KDrive")
    print("\n\u2705 Uniform scientific ontology processing completed!")